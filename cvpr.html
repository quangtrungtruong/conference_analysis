<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Phrase</th>
      <th>Frequencies</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>high fidelity text to 3d generation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>view synthesis from a single image</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>for text to image diffusion models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>for text to video diffusion models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>instance learning for whole slide image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>5</th>
      <td>learning for whole slide image classification</td>
      <td>2</td>
    </tr>
    <tr>
      <th>6</th>
      <td>text to image diffusion model with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>7</th>
      <td>in text to image generative models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>8</th>
      <td>end to end temporal action detection</td>
      <td>2</td>
    </tr>
    <tr>
      <th>9</th>
      <td>for cross domain few shot learning</td>
      <td>2</td>
    </tr>
    <tr>
      <th>10</th>
      <td>personalization of text to image models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>11</th>
      <td>for non exemplar class incremental learning</td>
      <td>2</td>
    </tr>
    <tr>
      <th>12</th>
      <td>slide representation learning in computational pathology</td>
      <td>2</td>
    </tr>
    <tr>
      <th>13</th>
      <td>foundation models for domain generalized semantic</td>
      <td>2</td>
    </tr>
    <tr>
      <th>14</th>
      <td>models for domain generalized semantic segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>15</th>
      <td>fidelity text to 3d generation via</td>
      <td>2</td>
    </tr>
    <tr>
      <th>16</th>
      <td>attention for unsupervised video object segmentation</td>
      <td>2</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Phrase</th>
      <th>Frequencies</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>text to image diffusion models</td>
      <td>7</td>
    </tr>
    <tr>
      <th>1</th>
      <td>text to 3d generation with</td>
      <td>6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>for text to image generation</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>for weakly supervised semantic segmentation</td>
      <td>6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>for open vocabulary object detection</td>
      <td>5</td>
    </tr>
    <tr>
      <th>5</th>
      <td>for cross domain few shot</td>
      <td>4</td>
    </tr>
    <tr>
      <th>6</th>
      <td>text to image diffusion model</td>
      <td>4</td>
    </tr>
    <tr>
      <th>7</th>
      <td>for semi supervised semantic segmentation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>8</th>
      <td>for human object interaction detection</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>high fidelity text to 3d</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>fidelity text to 3d generation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>human pose and shape estimation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>cross domain few shot segmentation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>multimodal large language model for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>text to image generative models</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>for out of distribution detection</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>for category agnostic pose estimation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>for text to 3d generation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>in large vision language models</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>text to 3d generation via</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>text to image generation with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>personalized text to image generation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>view synthesis from a single</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>synthesis from a single image</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>for open vocabulary semantic segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>25</th>
      <td>for text to image diffusion</td>
      <td>2</td>
    </tr>
    <tr>
      <th>26</th>
      <td>for text to video diffusion</td>
      <td>2</td>
    </tr>
    <tr>
      <th>27</th>
      <td>text to video diffusion models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>28</th>
      <td>moment retrieval and highlight detection</td>
      <td>2</td>
    </tr>
    <tr>
      <th>29</th>
      <td>trajectory prediction in autonomous driving</td>
      <td>2</td>
    </tr>
    <tr>
      <th>30</th>
      <td>instance learning for whole slide</td>
      <td>2</td>
    </tr>
    <tr>
      <th>31</th>
      <td>learning for whole slide image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>32</th>
      <td>for whole slide image classification</td>
      <td>2</td>
    </tr>
    <tr>
      <th>33</th>
      <td>to image diffusion model with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>34</th>
      <td>in text to image generative</td>
      <td>2</td>
    </tr>
    <tr>
      <th>35</th>
      <td>weakly supervised 3d object detection</td>
      <td>2</td>
    </tr>
    <tr>
      <th>36</th>
      <td>end to end temporal action</td>
      <td>2</td>
    </tr>
    <tr>
      <th>37</th>
      <td>to end temporal action detection</td>
      <td>2</td>
    </tr>
    <tr>
      <th>38</th>
      <td>for text to video generation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>39</th>
      <td>for vision language models a</td>
      <td>2</td>
    </tr>
    <tr>
      <th>40</th>
      <td>few shot object detection with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>41</th>
      <td>3d point cloud semantic segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>42</th>
      <td>cross domain few shot learning</td>
      <td>2</td>
    </tr>
    <tr>
      <th>43</th>
      <td>personalization of text to image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>44</th>
      <td>of text to image models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>45</th>
      <td>for zero shot video editing</td>
      <td>2</td>
    </tr>
    <tr>
      <th>46</th>
      <td>from a single image gaussian</td>
      <td>2</td>
    </tr>
    <tr>
      <th>47</th>
      <td>learning for multimodal large language</td>
      <td>2</td>
    </tr>
    <tr>
      <th>48</th>
      <td>for multimodal large language models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>49</th>
      <td>for text to image synthesis</td>
      <td>2</td>
    </tr>
    <tr>
      <th>50</th>
      <td>for non exemplar class incremental</td>
      <td>2</td>
    </tr>
    <tr>
      <th>51</th>
      <td>non exemplar class incremental learning</td>
      <td>2</td>
    </tr>
    <tr>
      <th>52</th>
      <td>of text to image diffusion</td>
      <td>2</td>
    </tr>
    <tr>
      <th>53</th>
      <td>mixture of low rank experts</td>
      <td>2</td>
    </tr>
    <tr>
      <th>54</th>
      <td>slide representation learning in computational</td>
      <td>2</td>
    </tr>
    <tr>
      <th>55</th>
      <td>representation learning in computational pathology</td>
      <td>2</td>
    </tr>
    <tr>
      <th>56</th>
      <td>from a single image with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>57</th>
      <td>generation with text to image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>58</th>
      <td>for low shot image classification</td>
      <td>2</td>
    </tr>
    <tr>
      <th>59</th>
      <td>an empirical study of the</td>
      <td>2</td>
    </tr>
    <tr>
      <th>60</th>
      <td>zero shot composed image retrieval</td>
      <td>2</td>
    </tr>
    <tr>
      <th>61</th>
      <td>model for 2d and 3d</td>
      <td>2</td>
    </tr>
    <tr>
      <th>62</th>
      <td>foundation models for domain generalized</td>
      <td>2</td>
    </tr>
    <tr>
      <th>63</th>
      <td>models for domain generalized semantic</td>
      <td>2</td>
    </tr>
    <tr>
      <th>64</th>
      <td>for domain generalized semantic segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>65</th>
      <td>with large language models for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>66</th>
      <td>all in one framework for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>67</th>
      <td>semi supervised medical image segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>68</th>
      <td>the devil is in the</td>
      <td>2</td>
    </tr>
    <tr>
      <th>69</th>
      <td>for zero shot adversarial robustness</td>
      <td>2</td>
    </tr>
    <tr>
      <th>70</th>
      <td>a large scale real world</td>
      <td>2</td>
    </tr>
    <tr>
      <th>71</th>
      <td>for 3d human pose estimation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>72</th>
      <td>via multimodal large language models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>73</th>
      <td>vision based 3d occupancy prediction</td>
      <td>2</td>
    </tr>
    <tr>
      <th>74</th>
      <td>in text to image models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>75</th>
      <td>visible infrared person re identification</td>
      <td>2</td>
    </tr>
    <tr>
      <th>76</th>
      <td>for vision language pre training</td>
      <td>2</td>
    </tr>
    <tr>
      <th>77</th>
      <td>attention for unsupervised video object</td>
      <td>2</td>
    </tr>
    <tr>
      <th>78</th>
      <td>for unsupervised video object segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>79</th>
      <td>for generalizable neural radiance fields</td>
      <td>2</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Phrase</th>
      <th>Frequencies</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>text to image generation</td>
      <td>18</td>
    </tr>
    <tr>
      <th>1</th>
      <td>text to image diffusion</td>
      <td>14</td>
    </tr>
    <tr>
      <th>2</th>
      <td>text to 3d generation</td>
      <td>13</td>
    </tr>
    <tr>
      <th>3</th>
      <td>for text to image</td>
      <td>12</td>
    </tr>
    <tr>
      <th>4</th>
      <td>from a single image</td>
      <td>9</td>
    </tr>
    <tr>
      <th>5</th>
      <td>open vocabulary object detection</td>
      <td>8</td>
    </tr>
    <tr>
      <th>6</th>
      <td>to image diffusion models</td>
      <td>7</td>
    </tr>
    <tr>
      <th>7</th>
      <td>weakly supervised semantic segmentation</td>
      <td>7</td>
    </tr>
    <tr>
      <th>8</th>
      <td>text to image models</td>
      <td>7</td>
    </tr>
    <tr>
      <th>9</th>
      <td>a unified framework for</td>
      <td>7</td>
    </tr>
    <tr>
      <th>10</th>
      <td>out of distribution detection</td>
      <td>6</td>
    </tr>
    <tr>
      <th>11</th>
      <td>to 3d generation with</td>
      <td>6</td>
    </tr>
    <tr>
      <th>12</th>
      <td>for text to video</td>
      <td>6</td>
    </tr>
    <tr>
      <th>13</th>
      <td>multimodal large language models</td>
      <td>6</td>
    </tr>
    <tr>
      <th>14</th>
      <td>cross domain few shot</td>
      <td>6</td>
    </tr>
    <tr>
      <th>15</th>
      <td>in text to image</td>
      <td>6</td>
    </tr>
    <tr>
      <th>16</th>
      <td>for vision language models</td>
      <td>6</td>
    </tr>
    <tr>
      <th>17</th>
      <td>for weakly supervised semantic</td>
      <td>6</td>
    </tr>
    <tr>
      <th>18</th>
      <td>3d human pose estimation</td>
      <td>6</td>
    </tr>
    <tr>
      <th>19</th>
      <td>3d object detection with</td>
      <td>5</td>
    </tr>
    <tr>
      <th>20</th>
      <td>open vocabulary semantic segmentation</td>
      <td>5</td>
    </tr>
    <tr>
      <th>21</th>
      <td>bird s eye view</td>
      <td>5</td>
    </tr>
    <tr>
      <th>22</th>
      <td>multimodal large language model</td>
      <td>5</td>
    </tr>
    <tr>
      <th>23</th>
      <td>test time adaptation for</td>
      <td>5</td>
    </tr>
    <tr>
      <th>24</th>
      <td>for open vocabulary object</td>
      <td>5</td>
    </tr>
    <tr>
      <th>25</th>
      <td>large language models for</td>
      <td>5</td>
    </tr>
    <tr>
      <th>26</th>
      <td>large vision language models</td>
      <td>5</td>
    </tr>
    <tr>
      <th>27</th>
      <td>monocular 3d object detection</td>
      <td>5</td>
    </tr>
    <tr>
      <th>28</th>
      <td>with vision language models</td>
      <td>4</td>
    </tr>
    <tr>
      <th>29</th>
      <td>for out of distribution</td>
      <td>4</td>
    </tr>
    <tr>
      <th>30</th>
      <td>human object interaction detection</td>
      <td>4</td>
    </tr>
    <tr>
      <th>31</th>
      <td>high fidelity text to</td>
      <td>4</td>
    </tr>
    <tr>
      <th>32</th>
      <td>for end to end</td>
      <td>4</td>
    </tr>
    <tr>
      <th>33</th>
      <td>for cross domain few</td>
      <td>4</td>
    </tr>
    <tr>
      <th>34</th>
      <td>of vision language models</td>
      <td>4</td>
    </tr>
    <tr>
      <th>35</th>
      <td>to image diffusion model</td>
      <td>4</td>
    </tr>
    <tr>
      <th>36</th>
      <td>in vision language models</td>
      <td>4</td>
    </tr>
    <tr>
      <th>37</th>
      <td>source free domain adaptation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>38</th>
      <td>an empirical study of</td>
      <td>4</td>
    </tr>
    <tr>
      <th>39</th>
      <td>vision language models for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>40</th>
      <td>of text to image</td>
      <td>4</td>
    </tr>
    <tr>
      <th>41</th>
      <td>for zero shot video</td>
      <td>4</td>
    </tr>
    <tr>
      <th>42</th>
      <td>text to image synthesis</td>
      <td>4</td>
    </tr>
    <tr>
      <th>43</th>
      <td>for image super resolution</td>
      <td>4</td>
    </tr>
    <tr>
      <th>44</th>
      <td>with large language models</td>
      <td>4</td>
    </tr>
    <tr>
      <th>45</th>
      <td>6d object pose estimation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>46</th>
      <td>for semi supervised semantic</td>
      <td>4</td>
    </tr>
    <tr>
      <th>47</th>
      <td>semi supervised semantic segmentation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>48</th>
      <td>for video question answering</td>
      <td>3</td>
    </tr>
    <tr>
      <th>49</th>
      <td>for human object interaction</td>
      <td>3</td>
    </tr>
    <tr>
      <th>50</th>
      <td>text to video diffusion</td>
      <td>3</td>
    </tr>
    <tr>
      <th>51</th>
      <td>to video diffusion models</td>
      <td>3</td>
    </tr>
    <tr>
      <th>52</th>
      <td>for neural radiance fields</td>
      <td>3</td>
    </tr>
    <tr>
      <th>53</th>
      <td>fidelity text to 3d</td>
      <td>3</td>
    </tr>
    <tr>
      <th>54</th>
      <td>one image to 3d</td>
      <td>3</td>
    </tr>
    <tr>
      <th>55</th>
      <td>human pose and shape</td>
      <td>3</td>
    </tr>
    <tr>
      <th>56</th>
      <td>pose and shape estimation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>57</th>
      <td>domain few shot segmentation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>58</th>
      <td>for 3d object detection</td>
      <td>3</td>
    </tr>
    <tr>
      <th>59</th>
      <td>3d gaussian splatting for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>60</th>
      <td>neural radiance fields from</td>
      <td>3</td>
    </tr>
    <tr>
      <th>61</th>
      <td>neural radiance fields with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>62</th>
      <td>object pose estimation via</td>
      <td>3</td>
    </tr>
    <tr>
      <th>63</th>
      <td>whole slide image classification</td>
      <td>3</td>
    </tr>
    <tr>
      <th>64</th>
      <td>large language model for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>65</th>
      <td>text to image generative</td>
      <td>3</td>
    </tr>
    <tr>
      <th>66</th>
      <td>to image generative models</td>
      <td>3</td>
    </tr>
    <tr>
      <th>67</th>
      <td>supervised 3d object detection</td>
      <td>3</td>
    </tr>
    <tr>
      <th>68</th>
      <td>text to video generation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>69</th>
      <td>for 3d visual grounding</td>
      <td>3</td>
    </tr>
    <tr>
      <th>70</th>
      <td>vision language pre training</td>
      <td>3</td>
    </tr>
    <tr>
      <th>71</th>
      <td>unleashing the power of</td>
      <td>3</td>
    </tr>
    <tr>
      <th>72</th>
      <td>point cloud semantic segmentation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>73</th>
      <td>text to image personalization</td>
      <td>3</td>
    </tr>
    <tr>
      <th>74</th>
      <td>for multimodal large language</td>
      <td>3</td>
    </tr>
    <tr>
      <th>75</th>
      <td>diffusion based image editing</td>
      <td>3</td>
    </tr>
    <tr>
      <th>76</th>
      <td>for monocular depth estimation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>77</th>
      <td>segment anything model for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>78</th>
      <td>for category agnostic pose</td>
      <td>3</td>
    </tr>
    <tr>
      <th>79</th>
      <td>category agnostic pose estimation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>80</th>
      <td>self supervised learning of</td>
      <td>3</td>
    </tr>
    <tr>
      <th>81</th>
      <td>single view 3d reconstruction</td>
      <td>3</td>
    </tr>
    <tr>
      <th>82</th>
      <td>for text to 3d</td>
      <td>3</td>
    </tr>
    <tr>
      <th>83</th>
      <td>for person re identification</td>
      <td>3</td>
    </tr>
    <tr>
      <th>84</th>
      <td>all you need for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>85</th>
      <td>video frame interpolation via</td>
      <td>3</td>
    </tr>
    <tr>
      <th>86</th>
      <td>for unsupervised domain adaptation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>87</th>
      <td>in large vision language</td>
      <td>3</td>
    </tr>
    <tr>
      <th>88</th>
      <td>to 3d generation via</td>
      <td>3</td>
    </tr>
    <tr>
      <th>89</th>
      <td>learning for medical image</td>
      <td>3</td>
    </tr>
    <tr>
      <th>90</th>
      <td>novel view synthesis with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>91</th>
      <td>to image generation with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>92</th>
      <td>self supervised learning for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>93</th>
      <td>free class incremental learning</td>
      <td>3</td>
    </tr>
    <tr>
      <th>94</th>
      <td>personalized text to image</td>
      <td>3</td>
    </tr>
    <tr>
      <th>95</th>
      <td>post training quantization for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>96</th>
      <td>view synthesis from a</td>
      <td>3</td>
    </tr>
    <tr>
      <th>97</th>
      <td>synthesis from a single</td>
      <td>3</td>
    </tr>
    <tr>
      <th>98</th>
      <td>from narrated egocentric videos</td>
      <td>2</td>
    </tr>
    <tr>
      <th>99</th>
      <td>with visual instruction tuning</td>
      <td>2</td>
    </tr>
    <tr>
      <th>100</th>
      <td>for open vocabulary semantic</td>
      <td>2</td>
    </tr>
    <tr>
      <th>101</th>
      <td>a unified model for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>102</th>
      <td>benchmark dataset and a</td>
      <td>2</td>
    </tr>
    <tr>
      <th>103</th>
      <td>large language models with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>104</th>
      <td>from pre trained models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>105</th>
      <td>generalizable face anti spoofing</td>
      <td>2</td>
    </tr>
    <tr>
      <th>106</th>
      <td>scene flow estimation with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>107</th>
      <td>vision language models on</td>
      <td>2</td>
    </tr>
    <tr>
      <th>108</th>
      <td>3d scene reconstruction with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>109</th>
      <td>skeleton based action recognition</td>
      <td>2</td>
    </tr>
    <tr>
      <th>110</th>
      <td>scale novel view synthesis</td>
      <td>2</td>
    </tr>
    <tr>
      <th>111</th>
      <td>scene text image synthesis</td>
      <td>2</td>
    </tr>
    <tr>
      <th>112</th>
      <td>synthesis with diffusion models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>113</th>
      <td>driven text to image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>114</th>
      <td>blind image super resolution</td>
      <td>2</td>
    </tr>
    <tr>
      <th>115</th>
      <td>moment retrieval and highlight</td>
      <td>2</td>
    </tr>
    <tr>
      <th>116</th>
      <td>retrieval and highlight detection</td>
      <td>2</td>
    </tr>
    <tr>
      <th>117</th>
      <td>in neural radiance fields</td>
      <td>2</td>
    </tr>
    <tr>
      <th>118</th>
      <td>dense video captioning with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>119</th>
      <td>for continual test time</td>
      <td>2</td>
    </tr>
    <tr>
      <th>120</th>
      <td>continual test time adaptation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>121</th>
      <td>in the physical world</td>
      <td>2</td>
    </tr>
    <tr>
      <th>122</th>
      <td>image representation learning for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>123</th>
      <td>vectorized hd map construction</td>
      <td>2</td>
    </tr>
    <tr>
      <th>124</th>
      <td>test time zero shot</td>
      <td>2</td>
    </tr>
    <tr>
      <th>125</th>
      <td>dataset and benchmark for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>126</th>
      <td>trajectory prediction in autonomous</td>
      <td>2</td>
    </tr>
    <tr>
      <th>127</th>
      <td>prediction in autonomous driving</td>
      <td>2</td>
    </tr>
    <tr>
      <th>128</th>
      <td>domain generalized object detection</td>
      <td>2</td>
    </tr>
    <tr>
      <th>129</th>
      <td>privacy preserving face recognition</td>
      <td>2</td>
    </tr>
    <tr>
      <th>130</th>
      <td>point based image editing</td>
      <td>2</td>
    </tr>
    <tr>
      <th>131</th>
      <td>of neural radiance fields</td>
      <td>2</td>
    </tr>
    <tr>
      <th>132</th>
      <td>multi person pose estimation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>133</th>
      <td>multiple instance learning for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>134</th>
      <td>instance learning for whole</td>
      <td>2</td>
    </tr>
    <tr>
      <th>135</th>
      <td>learning for whole slide</td>
      <td>2</td>
    </tr>
    <tr>
      <th>136</th>
      <td>for whole slide image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>137</th>
      <td>neural implicit representation for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>138</th>
      <td>for audio visual segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>139</th>
      <td>face anti spoofing via</td>
      <td>2</td>
    </tr>
    <tr>
      <th>140</th>
      <td>a diffusion model for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>141</th>
      <td>relightable and animatable neural</td>
      <td>2</td>
    </tr>
    <tr>
      <th>142</th>
      <td>image diffusion model with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>143</th>
      <td>robust test time adaptation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>144</th>
      <td>weakly supervised 3d object</td>
      <td>2</td>
    </tr>
    <tr>
      <th>145</th>
      <td>for semantic image synthesis</td>
      <td>2</td>
    </tr>
    <tr>
      <th>146</th>
      <td>end to end temporal</td>
      <td>2</td>
    </tr>
    <tr>
      <th>147</th>
      <td>to end temporal action</td>
      <td>2</td>
    </tr>
    <tr>
      <th>148</th>
      <td>end temporal action detection</td>
      <td>2</td>
    </tr>
    <tr>
      <th>149</th>
      <td>vision language models a</td>
      <td>2</td>
    </tr>
    <tr>
      <th>150</th>
      <td>a simple and effective</td>
      <td>2</td>
    </tr>
    <tr>
      <th>151</th>
      <td>few shot object detection</td>
      <td>2</td>
    </tr>
    <tr>
      <th>152</th>
      <td>shot object detection with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>153</th>
      <td>efficient text to image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>154</th>
      <td>for scene graph generation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>155</th>
      <td>aware hand object interaction</td>
      <td>2</td>
    </tr>
    <tr>
      <th>156</th>
      <td>3d semantic segmentation with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>157</th>
      <td>for visual place recognition</td>
      <td>2</td>
    </tr>
    <tr>
      <th>158</th>
      <td>3d point cloud semantic</td>
      <td>2</td>
    </tr>
    <tr>
      <th>159</th>
      <td>domain few shot learning</td>
      <td>2</td>
    </tr>
    <tr>
      <th>160</th>
      <td>for scene text recognition</td>
      <td>2</td>
    </tr>
    <tr>
      <th>161</th>
      <td>vision language model for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>162</th>
      <td>semi supervised object detection</td>
      <td>2</td>
    </tr>
    <tr>
      <th>163</th>
      <td>egocentric procedural task videos</td>
      <td>2</td>
    </tr>
    <tr>
      <th>164</th>
      <td>object detection with 2d</td>
      <td>2</td>
    </tr>
    <tr>
      <th>165</th>
      <td>from a single video</td>
      <td>2</td>
    </tr>
    <tr>
      <th>166</th>
      <td>personalization of text to</td>
      <td>2</td>
    </tr>
    <tr>
      <th>167</th>
      <td>co speech gesture generation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>168</th>
      <td>for pedestrian trajectory prediction</td>
      <td>2</td>
    </tr>
    <tr>
      <th>169</th>
      <td>for image to video</td>
      <td>2</td>
    </tr>
    <tr>
      <th>170</th>
      <td>image super resolution in</td>
      <td>2</td>
    </tr>
    <tr>
      <th>171</th>
      <td>zero shot video editing</td>
      <td>2</td>
    </tr>
    <tr>
      <th>172</th>
      <td>an end to end</td>
      <td>2</td>
    </tr>
    <tr>
      <th>173</th>
      <td>on the robustness of</td>
      <td>2</td>
    </tr>
    <tr>
      <th>174</th>
      <td>novel object pose estimation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>175</th>
      <td>a single image gaussian</td>
      <td>2</td>
    </tr>
    <tr>
      <th>176</th>
      <td>learning for multimodal large</td>
      <td>2</td>
    </tr>
    <tr>
      <th>177</th>
      <td>monocular metric depth estimation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>178</th>
      <td>for efficient and accurate</td>
      <td>2</td>
    </tr>
    <tr>
      <th>179</th>
      <td>for open world 3d</td>
      <td>2</td>
    </tr>
    <tr>
      <th>180</th>
      <td>for non exemplar class</td>
      <td>2</td>
    </tr>
    <tr>
      <th>181</th>
      <td>non exemplar class incremental</td>
      <td>2</td>
    </tr>
    <tr>
      <th>182</th>
      <td>exemplar class incremental learning</td>
      <td>2</td>
    </tr>
    <tr>
      <th>183</th>
      <td>large scale dataset and</td>
      <td>2</td>
    </tr>
    <tr>
      <th>184</th>
      <td>for segment anything model</td>
      <td>2</td>
    </tr>
    <tr>
      <th>185</th>
      <td>of diffusion models for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>186</th>
      <td>diffusion for zero shot</td>
      <td>2</td>
    </tr>
    <tr>
      <th>187</th>
      <td>in 3d scenes from</td>
      <td>2</td>
    </tr>
    <tr>
      <th>188</th>
      <td>image super resolution with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>189</th>
      <td>blind image quality assessment</td>
      <td>2</td>
    </tr>
    <tr>
      <th>190</th>
      <td>for long video understanding</td>
      <td>2</td>
    </tr>
    <tr>
      <th>191</th>
      <td>consistent novel view synthesis</td>
      <td>2</td>
    </tr>
    <tr>
      <th>192</th>
      <td>multi task dense prediction</td>
      <td>2</td>
    </tr>
    <tr>
      <th>193</th>
      <td>mixture of low rank</td>
      <td>2</td>
    </tr>
    <tr>
      <th>194</th>
      <td>of low rank experts</td>
      <td>2</td>
    </tr>
    <tr>
      <th>195</th>
      <td>slide representation learning in</td>
      <td>2</td>
    </tr>
    <tr>
      <th>196</th>
      <td>representation learning in computational</td>
      <td>2</td>
    </tr>
    <tr>
      <th>197</th>
      <td>learning in computational pathology</td>
      <td>2</td>
    </tr>
    <tr>
      <th>198</th>
      <td>a single image with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>199</th>
      <td>stable diffusion for text</td>
      <td>2</td>
    </tr>
    <tr>
      <th>200</th>
      <td>generation with text to</td>
      <td>2</td>
    </tr>
    <tr>
      <th>201</th>
      <td>with text to image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>202</th>
      <td>a large scale dataset</td>
      <td>2</td>
    </tr>
    <tr>
      <th>203</th>
      <td>3d hand pose estimation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>204</th>
      <td>pose estimation with diffusion</td>
      <td>2</td>
    </tr>
    <tr>
      <th>205</th>
      <td>transformers for 3d human</td>
      <td>2</td>
    </tr>
    <tr>
      <th>206</th>
      <td>for 3d human motion</td>
      <td>2</td>
    </tr>
    <tr>
      <th>207</th>
      <td>novel view synthesis from</td>
      <td>2</td>
    </tr>
    <tr>
      <th>208</th>
      <td>category level 6d object</td>
      <td>2</td>
    </tr>
    <tr>
      <th>209</th>
      <td>for high dynamic range</td>
      <td>2</td>
    </tr>
    <tr>
      <th>210</th>
      <td>for referring image segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>211</th>
      <td>learning for weakly supervised</td>
      <td>2</td>
    </tr>
    <tr>
      <th>212</th>
      <td>defense against adversarial attacks</td>
      <td>2</td>
    </tr>
    <tr>
      <th>213</th>
      <td>for low shot image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>214</th>
      <td>low shot image classification</td>
      <td>2</td>
    </tr>
    <tr>
      <th>215</th>
      <td>unveiling the power of</td>
      <td>2</td>
    </tr>
    <tr>
      <th>216</th>
      <td>benchmarking the robustness of</td>
      <td>2</td>
    </tr>
    <tr>
      <th>217</th>
      <td>is all you need</td>
      <td>2</td>
    </tr>
    <tr>
      <th>218</th>
      <td>multi object tracking with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>219</th>
      <td>adaptation for event based</td>
      <td>2</td>
    </tr>
    <tr>
      <th>220</th>
      <td>for text based image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>221</th>
      <td>end to end driving</td>
      <td>2</td>
    </tr>
    <tr>
      <th>222</th>
      <td>empirical study of the</td>
      <td>2</td>
    </tr>
    <tr>
      <th>223</th>
      <td>zero shot composed image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>224</th>
      <td>shot composed image retrieval</td>
      <td>2</td>
    </tr>
    <tr>
      <th>225</th>
      <td>for few shot image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>226</th>
      <td>3d human avatar generation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>227</th>
      <td>model for 2d and</td>
      <td>2</td>
    </tr>
    <tr>
      <th>228</th>
      <td>for 2d and 3d</td>
      <td>2</td>
    </tr>
    <tr>
      <th>229</th>
      <td>video to video synthesis</td>
      <td>2</td>
    </tr>
    <tr>
      <th>230</th>
      <td>vision foundation models for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>231</th>
      <td>foundation models for domain</td>
      <td>2</td>
    </tr>
    <tr>
      <th>232</th>
      <td>models for domain generalized</td>
      <td>2</td>
    </tr>
    <tr>
      <th>233</th>
      <td>for domain generalized semantic</td>
      <td>2</td>
    </tr>
    <tr>
      <th>234</th>
      <td>domain generalized semantic segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>235</th>
      <td>for test time adaptation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>236</th>
      <td>long term 3d human</td>
      <td>2</td>
    </tr>
    <tr>
      <th>237</th>
      <td>estimation with diffusion models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>238</th>
      <td>for blind motion deblurring</td>
      <td>2</td>
    </tr>
    <tr>
      <th>239</th>
      <td>end to end multi</td>
      <td>2</td>
    </tr>
    <tr>
      <th>240</th>
      <td>latent diffusion model for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>241</th>
      <td>invariant point cloud analysis</td>
      <td>2</td>
    </tr>
    <tr>
      <th>242</th>
      <td>for medical image segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>243</th>
      <td>open vocabulary 3d scene</td>
      <td>2</td>
    </tr>
    <tr>
      <th>244</th>
      <td>via large language model</td>
      <td>2</td>
    </tr>
    <tr>
      <th>245</th>
      <td>diffusion for single image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>246</th>
      <td>with fine grained visual</td>
      <td>2</td>
    </tr>
    <tr>
      <th>247</th>
      <td>in diffusion models for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>248</th>
      <td>vision language models with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>249</th>
      <td>all in one framework</td>
      <td>2</td>
    </tr>
    <tr>
      <th>250</th>
      <td>in one framework for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>251</th>
      <td>semi supervised medical image</td>
      <td>2</td>
    </tr>
    <tr>
      <th>252</th>
      <td>supervised medical image segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>253</th>
      <td>for zero shot semantic</td>
      <td>2</td>
    </tr>
    <tr>
      <th>254</th>
      <td>compositional zero shot learning</td>
      <td>2</td>
    </tr>
    <tr>
      <th>255</th>
      <td>the devil is in</td>
      <td>2</td>
    </tr>
    <tr>
      <th>256</th>
      <td>devil is in the</td>
      <td>2</td>
    </tr>
    <tr>
      <th>257</th>
      <td>for camera based 3d</td>
      <td>2</td>
    </tr>
    <tr>
      <th>258</th>
      <td>3d semantic scene completion</td>
      <td>2</td>
    </tr>
    <tr>
      <th>259</th>
      <td>supervised monocular depth estimation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>260</th>
      <td>for zero shot adversarial</td>
      <td>2</td>
    </tr>
    <tr>
      <th>261</th>
      <td>zero shot adversarial robustness</td>
      <td>2</td>
    </tr>
    <tr>
      <th>262</th>
      <td>text to image person</td>
      <td>2</td>
    </tr>
    <tr>
      <th>263</th>
      <td>a large scale real</td>
      <td>2</td>
    </tr>
    <tr>
      <th>264</th>
      <td>large scale real world</td>
      <td>2</td>
    </tr>
    <tr>
      <th>265</th>
      <td>multi modal large language</td>
      <td>2</td>
    </tr>
    <tr>
      <th>266</th>
      <td>large language model with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>267</th>
      <td>for 3d human pose</td>
      <td>2</td>
    </tr>
    <tr>
      <th>268</th>
      <td>parameter efficient learning for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>269</th>
      <td>point cloud instance segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>270</th>
      <td>for scene text detection</td>
      <td>2</td>
    </tr>
    <tr>
      <th>271</th>
      <td>data free knowledge distillation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>272</th>
      <td>for vision based 3d</td>
      <td>2</td>
    </tr>
    <tr>
      <th>273</th>
      <td>diffusion models for high</td>
      <td>2</td>
    </tr>
    <tr>
      <th>274</th>
      <td>via multimodal large language</td>
      <td>2</td>
    </tr>
    <tr>
      <th>275</th>
      <td>large multi modal models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>276</th>
      <td>semantic scene completion with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>277</th>
      <td>for event based object</td>
      <td>2</td>
    </tr>
    <tr>
      <th>278</th>
      <td>super resolution transformer with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>279</th>
      <td>3d gaussian splatting with</td>
      <td>2</td>
    </tr>
    <tr>
      <th>280</th>
      <td>for source free domain</td>
      <td>2</td>
    </tr>
    <tr>
      <th>281</th>
      <td>for video frame interpolation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>282</th>
      <td>guided human motion generation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>283</th>
      <td>vision based 3d occupancy</td>
      <td>2</td>
    </tr>
    <tr>
      <th>284</th>
      <td>based 3d occupancy prediction</td>
      <td>2</td>
    </tr>
    <tr>
      <th>285</th>
      <td>in neural radiance field</td>
      <td>2</td>
    </tr>
    <tr>
      <th>286</th>
      <td>with large language model</td>
      <td>2</td>
    </tr>
    <tr>
      <th>287</th>
      <td>few shot semantic segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>288</th>
      <td>vision and language navigation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>289</th>
      <td>unlocking the potential of</td>
      <td>2</td>
    </tr>
    <tr>
      <th>290</th>
      <td>visible infrared person re</td>
      <td>2</td>
    </tr>
    <tr>
      <th>291</th>
      <td>infrared person re identification</td>
      <td>2</td>
    </tr>
    <tr>
      <th>292</th>
      <td>quantization for diffusion models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>293</th>
      <td>single image to 3d</td>
      <td>2</td>
    </tr>
    <tr>
      <th>294</th>
      <td>3d human generation via</td>
      <td>2</td>
    </tr>
    <tr>
      <th>295</th>
      <td>multi modal language models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>296</th>
      <td>universal medical image segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>297</th>
      <td>spatio temporal video grounding</td>
      <td>2</td>
    </tr>
    <tr>
      <th>298</th>
      <td>for vision language pre</td>
      <td>2</td>
    </tr>
    <tr>
      <th>299</th>
      <td>attacks on object detection</td>
      <td>2</td>
    </tr>
    <tr>
      <th>300</th>
      <td>attention for unsupervised video</td>
      <td>2</td>
    </tr>
    <tr>
      <th>301</th>
      <td>for unsupervised video object</td>
      <td>2</td>
    </tr>
    <tr>
      <th>302</th>
      <td>unsupervised video object segmentation</td>
      <td>2</td>
    </tr>
    <tr>
      <th>303</th>
      <td>for generalizable neural radiance</td>
      <td>2</td>
    </tr>
    <tr>
      <th>304</th>
      <td>generalizable neural radiance fields</td>
      <td>2</td>
    </tr>
    <tr>
      <th>305</th>
      <td>loop end to end</td>
      <td>2</td>
    </tr>
    <tr>
      <th>306</th>
      <td>a simple yet effective</td>
      <td>2</td>
    </tr>
    <tr>
      <th>307</th>
      <td>generation via multi view</td>
      <td>2</td>
    </tr>
    <tr>
      <th>308</th>
      <td>human pose estimation via</td>
      <td>2</td>
    </tr>
    <tr>
      <th>309</th>
      <td>text to 3d using</td>
      <td>2</td>
    </tr>
    <tr>
      <th>310</th>
      <td>multi modal learning for</td>
      <td>2</td>
    </tr>
    <tr>
      <th>311</th>
      <td>free 3d gaussian splatting</td>
      <td>2</td>
    </tr>
    <tr>
      <th>312</th>
      <td>for weakly supervised video</td>
      <td>2</td>
    </tr>
    <tr>
      <th>313</th>
      <td>lifelong person re identification</td>
      <td>2</td>
    </tr>
    <tr>
      <th>314</th>
      <td>bridging the gap between</td>
      <td>2</td>
    </tr>
    <tr>
      <th>315</th>
      <td>to 3d using cross</td>
      <td>2</td>
    </tr>
    <tr>
      <th>316</th>
      <td>of 3d human motions</td>
      <td>2</td>
    </tr>
    <tr>
      <th>317</th>
      <td>for occluded human mesh</td>
      <td>2</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Phrase</th>
      <th>Frequencies</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>text to image</td>
      <td>54</td>
    </tr>
    <tr>
      <th>1</th>
      <td>vision language models</td>
      <td>36</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3d object detection</td>
      <td>24</td>
    </tr>
    <tr>
      <th>3</th>
      <td>for text to</td>
      <td>23</td>
    </tr>
    <tr>
      <th>4</th>
      <td>large language models</td>
      <td>20</td>
    </tr>
    <tr>
      <th>5</th>
      <td>end to end</td>
      <td>19</td>
    </tr>
    <tr>
      <th>6</th>
      <td>neural radiance fields</td>
      <td>18</td>
    </tr>
    <tr>
      <th>7</th>
      <td>to image generation</td>
      <td>18</td>
    </tr>
    <tr>
      <th>8</th>
      <td>for zero shot</td>
      <td>17</td>
    </tr>
    <tr>
      <th>9</th>
      <td>text to 3d</td>
      <td>17</td>
    </tr>
    <tr>
      <th>10</th>
      <td>diffusion models for</td>
      <td>16</td>
    </tr>
    <tr>
      <th>11</th>
      <td>diffusion model for</td>
      <td>16</td>
    </tr>
    <tr>
      <th>12</th>
      <td>image super resolution</td>
      <td>16</td>
    </tr>
    <tr>
      <th>13</th>
      <td>to image diffusion</td>
      <td>14</td>
    </tr>
    <tr>
      <th>14</th>
      <td>to 3d generation</td>
      <td>14</td>
    </tr>
    <tr>
      <th>15</th>
      <td>test time adaptation</td>
      <td>14</td>
    </tr>
    <tr>
      <th>16</th>
      <td>from a single</td>
      <td>14</td>
    </tr>
    <tr>
      <th>17</th>
      <td>for weakly supervised</td>
      <td>13</td>
    </tr>
    <tr>
      <th>18</th>
      <td>supervised semantic segmentation</td>
      <td>12</td>
    </tr>
    <tr>
      <th>19</th>
      <td>out of distribution</td>
      <td>12</td>
    </tr>
    <tr>
      <th>20</th>
      <td>large language model</td>
      <td>12</td>
    </tr>
    <tr>
      <th>21</th>
      <td>self supervised learning</td>
      <td>12</td>
    </tr>
    <tr>
      <th>22</th>
      <td>object detection with</td>
      <td>11</td>
    </tr>
    <tr>
      <th>23</th>
      <td>for open vocabulary</td>
      <td>11</td>
    </tr>
    <tr>
      <th>24</th>
      <td>for few shot</td>
      <td>11</td>
    </tr>
    <tr>
      <th>25</th>
      <td>multimodal large language</td>
      <td>11</td>
    </tr>
    <tr>
      <th>26</th>
      <td>class incremental learning</td>
      <td>11</td>
    </tr>
    <tr>
      <th>27</th>
      <td>of diffusion models</td>
      <td>11</td>
    </tr>
    <tr>
      <th>28</th>
      <td>person re identification</td>
      <td>11</td>
    </tr>
    <tr>
      <th>29</th>
      <td>language models for</td>
      <td>11</td>
    </tr>
    <tr>
      <th>30</th>
      <td>with diffusion models</td>
      <td>10</td>
    </tr>
    <tr>
      <th>31</th>
      <td>open vocabulary object</td>
      <td>10</td>
    </tr>
    <tr>
      <th>32</th>
      <td>medical image segmentation</td>
      <td>10</td>
    </tr>
    <tr>
      <th>33</th>
      <td>in the wild</td>
      <td>10</td>
    </tr>
    <tr>
      <th>34</th>
      <td>3d gaussian splatting</td>
      <td>10</td>
    </tr>
    <tr>
      <th>35</th>
      <td>a large scale</td>
      <td>10</td>
    </tr>
    <tr>
      <th>36</th>
      <td>for autonomous driving</td>
      <td>9</td>
    </tr>
    <tr>
      <th>37</th>
      <td>novel view synthesis</td>
      <td>9</td>
    </tr>
    <tr>
      <th>38</th>
      <td>text to video</td>
      <td>9</td>
    </tr>
    <tr>
      <th>39</th>
      <td>object pose estimation</td>
      <td>9</td>
    </tr>
    <tr>
      <th>40</th>
      <td>for vision language</td>
      <td>9</td>
    </tr>
    <tr>
      <th>41</th>
      <td>a single image</td>
      <td>9</td>
    </tr>
    <tr>
      <th>42</th>
      <td>for semi supervised</td>
      <td>9</td>
    </tr>
    <tr>
      <th>43</th>
      <td>segment anything model</td>
      <td>9</td>
    </tr>
    <tr>
      <th>44</th>
      <td>3d human pose</td>
      <td>9</td>
    </tr>
    <tr>
      <th>45</th>
      <td>image diffusion models</td>
      <td>8</td>
    </tr>
    <tr>
      <th>46</th>
      <td>vocabulary object detection</td>
      <td>8</td>
    </tr>
    <tr>
      <th>47</th>
      <td>human object interaction</td>
      <td>8</td>
    </tr>
    <tr>
      <th>48</th>
      <td>gaussian splatting for</td>
      <td>8</td>
    </tr>
    <tr>
      <th>49</th>
      <td>human pose estimation</td>
      <td>8</td>
    </tr>
    <tr>
      <th>50</th>
      <td>a unified framework</td>
      <td>8</td>
    </tr>
    <tr>
      <th>51</th>
      <td>in autonomous driving</td>
      <td>7</td>
    </tr>
    <tr>
      <th>52</th>
      <td>structure from motion</td>
      <td>7</td>
    </tr>
    <tr>
      <th>53</th>
      <td>video diffusion models</td>
      <td>7</td>
    </tr>
    <tr>
      <th>54</th>
      <td>representation learning for</td>
      <td>7</td>
    </tr>
    <tr>
      <th>55</th>
      <td>based image editing</td>
      <td>7</td>
    </tr>
    <tr>
      <th>56</th>
      <td>few shot segmentation</td>
      <td>7</td>
    </tr>
    <tr>
      <th>57</th>
      <td>in vision language</td>
      <td>7</td>
    </tr>
    <tr>
      <th>58</th>
      <td>for diffusion models</td>
      <td>7</td>
    </tr>
    <tr>
      <th>59</th>
      <td>language model for</td>
      <td>7</td>
    </tr>
    <tr>
      <th>60</th>
      <td>3d point cloud</td>
      <td>7</td>
    </tr>
    <tr>
      <th>61</th>
      <td>video object segmentation</td>
      <td>7</td>
    </tr>
    <tr>
      <th>62</th>
      <td>in text to</td>
      <td>7</td>
    </tr>
    <tr>
      <th>63</th>
      <td>contrastive learning for</td>
      <td>7</td>
    </tr>
    <tr>
      <th>64</th>
      <td>in diffusion models</td>
      <td>7</td>
    </tr>
    <tr>
      <th>65</th>
      <td>the power of</td>
      <td>7</td>
    </tr>
    <tr>
      <th>66</th>
      <td>weakly supervised semantic</td>
      <td>7</td>
    </tr>
    <tr>
      <th>67</th>
      <td>to image models</td>
      <td>7</td>
    </tr>
    <tr>
      <th>68</th>
      <td>unified framework for</td>
      <td>7</td>
    </tr>
    <tr>
      <th>69</th>
      <td>video frame interpolation</td>
      <td>7</td>
    </tr>
    <tr>
      <th>70</th>
      <td>scene graph generation</td>
      <td>6</td>
    </tr>
    <tr>
      <th>71</th>
      <td>open vocabulary semantic</td>
      <td>6</td>
    </tr>
    <tr>
      <th>72</th>
      <td>for real world</td>
      <td>6</td>
    </tr>
    <tr>
      <th>73</th>
      <td>of distribution detection</td>
      <td>6</td>
    </tr>
    <tr>
      <th>74</th>
      <td>prompt learning for</td>
      <td>6</td>
    </tr>
    <tr>
      <th>75</th>
      <td>face anti spoofing</td>
      <td>6</td>
    </tr>
    <tr>
      <th>76</th>
      <td>3d generation with</td>
      <td>6</td>
    </tr>
    <tr>
      <th>77</th>
      <td>few shot learning</td>
      <td>6</td>
    </tr>
    <tr>
      <th>78</th>
      <td>zero shot learning</td>
      <td>6</td>
    </tr>
    <tr>
      <th>79</th>
      <td>cross domain few</td>
      <td>6</td>
    </tr>
    <tr>
      <th>80</th>
      <td>domain few shot</td>
      <td>6</td>
    </tr>
    <tr>
      <th>81</th>
      <td>pose estimation via</td>
      <td>6</td>
    </tr>
    <tr>
      <th>82</th>
      <td>for 3d human</td>
      <td>6</td>
    </tr>
    <tr>
      <th>83</th>
      <td>pose estimation with</td>
      <td>6</td>
    </tr>
    <tr>
      <th>84</th>
      <td>plug and play</td>
      <td>6</td>
    </tr>
    <tr>
      <th>85</th>
      <td>3d visual grounding</td>
      <td>6</td>
    </tr>
    <tr>
      <th>86</th>
      <td>video anomaly detection</td>
      <td>6</td>
    </tr>
    <tr>
      <th>87</th>
      <td>for open world</td>
      <td>6</td>
    </tr>
    <tr>
      <th>88</th>
      <td>monocular depth estimation</td>
      <td>6</td>
    </tr>
    <tr>
      <th>89</th>
      <td>image generation with</td>
      <td>6</td>
    </tr>
    <tr>
      <th>90</th>
      <td>3d gaussians for</td>
      <td>6</td>
    </tr>
    <tr>
      <th>91</th>
      <td>for high fidelity</td>
      <td>6</td>
    </tr>
    <tr>
      <th>92</th>
      <td>with large language</td>
      <td>6</td>
    </tr>
    <tr>
      <th>93</th>
      <td>large vision language</td>
      <td>6</td>
    </tr>
    <tr>
      <th>94</th>
      <td>with vision language</td>
      <td>5</td>
    </tr>
    <tr>
      <th>95</th>
      <td>point cloud registration</td>
      <td>5</td>
    </tr>
    <tr>
      <th>96</th>
      <td>for fine grained</td>
      <td>5</td>
    </tr>
    <tr>
      <th>97</th>
      <td>vocabulary semantic segmentation</td>
      <td>5</td>
    </tr>
    <tr>
      <th>98</th>
      <td>language models with</td>
      <td>5</td>
    </tr>
    <tr>
      <th>99</th>
      <td>vision and language</td>
      <td>5</td>
    </tr>
    <tr>
      <th>100</th>
      <td>video question answering</td>
      <td>5</td>
    </tr>
    <tr>
      <th>101</th>
      <td>bird s eye</td>
      <td>5</td>
    </tr>
    <tr>
      <th>102</th>
      <td>s eye view</td>
      <td>5</td>
    </tr>
    <tr>
      <th>103</th>
      <td>for large scale</td>
      <td>5</td>
    </tr>
    <tr>
      <th>104</th>
      <td>image to 3d</td>
      <td>5</td>
    </tr>
    <tr>
      <th>105</th>
      <td>all in one</td>
      <td>5</td>
    </tr>
    <tr>
      <th>106</th>
      <td>for cross domain</td>
      <td>5</td>
    </tr>
    <tr>
      <th>107</th>
      <td>text guided 3d</td>
      <td>5</td>
    </tr>
    <tr>
      <th>108</th>
      <td>3d scene understanding</td>
      <td>5</td>
    </tr>
    <tr>
      <th>109</th>
      <td>radiance fields from</td>
      <td>5</td>
    </tr>
    <tr>
      <th>110</th>
      <td>in 3d scenes</td>
      <td>5</td>
    </tr>
    <tr>
      <th>111</th>
      <td>prompt tuning for</td>
      <td>5</td>
    </tr>
    <tr>
      <th>112</th>
      <td>coarse to fine</td>
      <td>5</td>
    </tr>
    <tr>
      <th>113</th>
      <td>vision language model</td>
      <td>5</td>
    </tr>
    <tr>
      <th>114</th>
      <td>source free domain</td>
      <td>5</td>
    </tr>
    <tr>
      <th>115</th>
      <td>an empirical study</td>
      <td>5</td>
    </tr>
    <tr>
      <th>116</th>
      <td>of text to</td>
      <td>5</td>
    </tr>
    <tr>
      <th>117</th>
      <td>time adaptation for</td>
      <td>5</td>
    </tr>
    <tr>
      <th>118</th>
      <td>the robustness of</td>
      <td>5</td>
    </tr>
    <tr>
      <th>119</th>
      <td>to image synthesis</td>
      <td>5</td>
    </tr>
    <tr>
      <th>120</th>
      <td>diffusion models with</td>
      <td>5</td>
    </tr>
    <tr>
      <th>121</th>
      <td>diffusion based image</td>
      <td>5</td>
    </tr>
    <tr>
      <th>122</th>
      <td>virtual try on</td>
      <td>5</td>
    </tr>
    <tr>
      <th>123</th>
      <td>multi object tracking</td>
      <td>5</td>
    </tr>
    <tr>
      <th>124</th>
      <td>monocular 3d object</td>
      <td>5</td>
    </tr>
    <tr>
      <th>125</th>
      <td>open vocabulary 3d</td>
      <td>5</td>
    </tr>
    <tr>
      <th>126</th>
      <td>for multi view</td>
      <td>4</td>
    </tr>
    <tr>
      <th>127</th>
      <td>of 3d human</td>
      <td>4</td>
    </tr>
    <tr>
      <th>128</th>
      <td>visual instruction tuning</td>
      <td>4</td>
    </tr>
    <tr>
      <th>129</th>
      <td>for stereo matching</td>
      <td>4</td>
    </tr>
    <tr>
      <th>130</th>
      <td>referring image segmentation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>131</th>
      <td>audio visual segmentation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>132</th>
      <td>hand object interaction</td>
      <td>4</td>
    </tr>
    <tr>
      <th>133</th>
      <td>pre trained models</td>
      <td>4</td>
    </tr>
    <tr>
      <th>134</th>
      <td>diffusion framework for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>135</th>
      <td>for self supervised</td>
      <td>4</td>
    </tr>
    <tr>
      <th>136</th>
      <td>for out of</td>
      <td>4</td>
    </tr>
    <tr>
      <th>137</th>
      <td>image to video</td>
      <td>4</td>
    </tr>
    <tr>
      <th>138</th>
      <td>to video synthesis</td>
      <td>4</td>
    </tr>
    <tr>
      <th>139</th>
      <td>pre training for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>140</th>
      <td>object interaction detection</td>
      <td>4</td>
    </tr>
    <tr>
      <th>141</th>
      <td>to video diffusion</td>
      <td>4</td>
    </tr>
    <tr>
      <th>142</th>
      <td>image synthesis with</td>
      <td>4</td>
    </tr>
    <tr>
      <th>143</th>
      <td>for neural radiance</td>
      <td>4</td>
    </tr>
    <tr>
      <th>144</th>
      <td>in neural radiance</td>
      <td>4</td>
    </tr>
    <tr>
      <th>145</th>
      <td>for category level</td>
      <td>4</td>
    </tr>
    <tr>
      <th>146</th>
      <td>high fidelity text</td>
      <td>4</td>
    </tr>
    <tr>
      <th>147</th>
      <td>fidelity text to</td>
      <td>4</td>
    </tr>
    <tr>
      <th>148</th>
      <td>for end to</td>
      <td>4</td>
    </tr>
    <tr>
      <th>149</th>
      <td>online continual learning</td>
      <td>4</td>
    </tr>
    <tr>
      <th>150</th>
      <td>foundation model for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>151</th>
      <td>knowledge distillation for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>152</th>
      <td>of vision language</td>
      <td>4</td>
    </tr>
    <tr>
      <th>153</th>
      <td>3d semantic segmentation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>154</th>
      <td>radiance fields with</td>
      <td>4</td>
    </tr>
    <tr>
      <th>155</th>
      <td>whole slide image</td>
      <td>4</td>
    </tr>
    <tr>
      <th>156</th>
      <td>reconstruction from monocular</td>
      <td>4</td>
    </tr>
    <tr>
      <th>157</th>
      <td>video quality assessment</td>
      <td>4</td>
    </tr>
    <tr>
      <th>158</th>
      <td>for single image</td>
      <td>4</td>
    </tr>
    <tr>
      <th>159</th>
      <td>3d instance segmentation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>160</th>
      <td>image diffusion model</td>
      <td>4</td>
    </tr>
    <tr>
      <th>161</th>
      <td>3d point clouds</td>
      <td>4</td>
    </tr>
    <tr>
      <th>162</th>
      <td>temporal action detection</td>
      <td>4</td>
    </tr>
    <tr>
      <th>163</th>
      <td>with foundation models</td>
      <td>4</td>
    </tr>
    <tr>
      <th>164</th>
      <td>language pre training</td>
      <td>4</td>
    </tr>
    <tr>
      <th>165</th>
      <td>neural radiance field</td>
      <td>4</td>
    </tr>
    <tr>
      <th>166</th>
      <td>for point cloud</td>
      <td>4</td>
    </tr>
    <tr>
      <th>167</th>
      <td>semantic segmentation with</td>
      <td>4</td>
    </tr>
    <tr>
      <th>168</th>
      <td>generation with diffusion</td>
      <td>4</td>
    </tr>
    <tr>
      <th>169</th>
      <td>for high resolution</td>
      <td>4</td>
    </tr>
    <tr>
      <th>170</th>
      <td>free domain adaptation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>171</th>
      <td>multi modal models</td>
      <td>4</td>
    </tr>
    <tr>
      <th>172</th>
      <td>empirical study of</td>
      <td>4</td>
    </tr>
    <tr>
      <th>173</th>
      <td>for scene text</td>
      <td>4</td>
    </tr>
    <tr>
      <th>174</th>
      <td>for event based</td>
      <td>4</td>
    </tr>
    <tr>
      <th>175</th>
      <td>co speech gesture</td>
      <td>4</td>
    </tr>
    <tr>
      <th>176</th>
      <td>in real world</td>
      <td>4</td>
    </tr>
    <tr>
      <th>177</th>
      <td>zero shot video</td>
      <td>4</td>
    </tr>
    <tr>
      <th>178</th>
      <td>learning for multimodal</td>
      <td>4</td>
    </tr>
    <tr>
      <th>179</th>
      <td>radiance fields for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>180</th>
      <td>for image super</td>
      <td>4</td>
    </tr>
    <tr>
      <th>181</th>
      <td>2d and 3d</td>
      <td>4</td>
    </tr>
    <tr>
      <th>182</th>
      <td>low light enhancement</td>
      <td>4</td>
    </tr>
    <tr>
      <th>183</th>
      <td>unsupervised domain adaptation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>184</th>
      <td>image quality assessment</td>
      <td>4</td>
    </tr>
    <tr>
      <th>185</th>
      <td>in context learning</td>
      <td>4</td>
    </tr>
    <tr>
      <th>186</th>
      <td>audio visual speech</td>
      <td>4</td>
    </tr>
    <tr>
      <th>187</th>
      <td>domain generalization in</td>
      <td>4</td>
    </tr>
    <tr>
      <th>188</th>
      <td>single view 3d</td>
      <td>4</td>
    </tr>
    <tr>
      <th>189</th>
      <td>view synthesis from</td>
      <td>4</td>
    </tr>
    <tr>
      <th>190</th>
      <td>implicit neural representation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>191</th>
      <td>from monocular videos</td>
      <td>4</td>
    </tr>
    <tr>
      <th>192</th>
      <td>all you need</td>
      <td>4</td>
    </tr>
    <tr>
      <th>193</th>
      <td>pre trained vision</td>
      <td>4</td>
    </tr>
    <tr>
      <th>194</th>
      <td>latent diffusion model</td>
      <td>4</td>
    </tr>
    <tr>
      <th>195</th>
      <td>for unsupervised domain</td>
      <td>4</td>
    </tr>
    <tr>
      <th>196</th>
      <td>6d object pose</td>
      <td>4</td>
    </tr>
    <tr>
      <th>197</th>
      <td>semi supervised semantic</td>
      <td>4</td>
    </tr>
    <tr>
      <th>198</th>
      <td>vision foundation models</td>
      <td>4</td>
    </tr>
    <tr>
      <th>199</th>
      <td>3d generation via</td>
      <td>4</td>
    </tr>
    <tr>
      <th>200</th>
      <td>for semantic segmentation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>201</th>
      <td>video super resolution</td>
      <td>4</td>
    </tr>
    <tr>
      <th>202</th>
      <td>for medical image</td>
      <td>4</td>
    </tr>
    <tr>
      <th>203</th>
      <td>video generation with</td>
      <td>4</td>
    </tr>
    <tr>
      <th>204</th>
      <td>semantic scene completion</td>
      <td>4</td>
    </tr>
    <tr>
      <th>205</th>
      <td>generalized category discovery</td>
      <td>4</td>
    </tr>
    <tr>
      <th>206</th>
      <td>3d human generation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>207</th>
      <td>post training quantization</td>
      <td>4</td>
    </tr>
    <tr>
      <th>208</th>
      <td>representation for image</td>
      <td>3</td>
    </tr>
    <tr>
      <th>209</th>
      <td>vision language reasoning</td>
      <td>3</td>
    </tr>
    <tr>
      <th>210</th>
      <td>pre training with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>211</th>
      <td>masked autoencoders for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>212</th>
      <td>for object detection</td>
      <td>3</td>
    </tr>
    <tr>
      <th>213</th>
      <td>for audio visual</td>
      <td>3</td>
    </tr>
    <tr>
      <th>214</th>
      <td>domain adaptation with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>215</th>
      <td>weakly supervised video</td>
      <td>3</td>
    </tr>
    <tr>
      <th>216</th>
      <td>visual object tracking</td>
      <td>3</td>
    </tr>
    <tr>
      <th>217</th>
      <td>benchmark dataset and</td>
      <td>3</td>
    </tr>
    <tr>
      <th>218</th>
      <td>for multi label</td>
      <td>3</td>
    </tr>
    <tr>
      <th>219</th>
      <td>referring expression comprehension</td>
      <td>3</td>
    </tr>
    <tr>
      <th>220</th>
      <td>scene flow estimation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>221</th>
      <td>flow estimation with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>222</th>
      <td>zero shot image</td>
      <td>3</td>
    </tr>
    <tr>
      <th>223</th>
      <td>vision language pretraining</td>
      <td>3</td>
    </tr>
    <tr>
      <th>224</th>
      <td>with multi view</td>
      <td>3</td>
    </tr>
    <tr>
      <th>225</th>
      <td>detection and localization</td>
      <td>3</td>
    </tr>
    <tr>
      <th>226</th>
      <td>3d scene reconstruction</td>
      <td>3</td>
    </tr>
    <tr>
      <th>227</th>
      <td>based action recognition</td>
      <td>3</td>
    </tr>
    <tr>
      <th>228</th>
      <td>a real world</td>
      <td>3</td>
    </tr>
    <tr>
      <th>229</th>
      <td>motion prediction with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>230</th>
      <td>for multi modal</td>
      <td>3</td>
    </tr>
    <tr>
      <th>231</th>
      <td>object level image</td>
      <td>3</td>
    </tr>
    <tr>
      <th>232</th>
      <td>image editing via</td>
      <td>3</td>
    </tr>
    <tr>
      <th>233</th>
      <td>dense video captioning</td>
      <td>3</td>
    </tr>
    <tr>
      <th>234</th>
      <td>for video question</td>
      <td>3</td>
    </tr>
    <tr>
      <th>235</th>
      <td>for human object</td>
      <td>3</td>
    </tr>
    <tr>
      <th>236</th>
      <td>image and video</td>
      <td>3</td>
    </tr>
    <tr>
      <th>237</th>
      <td>semantic segmentation from</td>
      <td>3</td>
    </tr>
    <tr>
      <th>238</th>
      <td>human reconstruction with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>239</th>
      <td>the potential of</td>
      <td>3</td>
    </tr>
    <tr>
      <th>240</th>
      <td>one image to</td>
      <td>3</td>
    </tr>
    <tr>
      <th>241</th>
      <td>continual test time</td>
      <td>3</td>
    </tr>
    <tr>
      <th>242</th>
      <td>human pose and</td>
      <td>3</td>
    </tr>
    <tr>
      <th>243</th>
      <td>pose and shape</td>
      <td>3</td>
    </tr>
    <tr>
      <th>244</th>
      <td>and shape estimation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>245</th>
      <td>for 3d object</td>
      <td>3</td>
    </tr>
    <tr>
      <th>246</th>
      <td>dataset and benchmark</td>
      <td>3</td>
    </tr>
    <tr>
      <th>247</th>
      <td>and benchmark for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>248</th>
      <td>point cloud upsampling</td>
      <td>3</td>
    </tr>
    <tr>
      <th>249</th>
      <td>chain of thought</td>
      <td>3</td>
    </tr>
    <tr>
      <th>250</th>
      <td>semantic occupancy prediction</td>
      <td>3</td>
    </tr>
    <tr>
      <th>251</th>
      <td>instance learning for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>252</th>
      <td>slide image classification</td>
      <td>3</td>
    </tr>
    <tr>
      <th>253</th>
      <td>a benchmark for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>254</th>
      <td>clothed human reconstruction</td>
      <td>3</td>
    </tr>
    <tr>
      <th>255</th>
      <td>for indoor scenes</td>
      <td>3</td>
    </tr>
    <tr>
      <th>256</th>
      <td>vision transformer for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>257</th>
      <td>detection with 2d</td>
      <td>3</td>
    </tr>
    <tr>
      <th>258</th>
      <td>to image generative</td>
      <td>3</td>
    </tr>
    <tr>
      <th>259</th>
      <td>image generative models</td>
      <td>3</td>
    </tr>
    <tr>
      <th>260</th>
      <td>supervised 3d object</td>
      <td>3</td>
    </tr>
    <tr>
      <th>261</th>
      <td>for image classification</td>
      <td>3</td>
    </tr>
    <tr>
      <th>262</th>
      <td>to video generation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>263</th>
      <td>efficient text to</td>
      <td>3</td>
    </tr>
    <tr>
      <th>264</th>
      <td>transformer for 3d</td>
      <td>3</td>
    </tr>
    <tr>
      <th>265</th>
      <td>for 3d visual</td>
      <td>3</td>
    </tr>
    <tr>
      <th>266</th>
      <td>language models a</td>
      <td>3</td>
    </tr>
    <tr>
      <th>267</th>
      <td>vision language pre</td>
      <td>3</td>
    </tr>
    <tr>
      <th>268</th>
      <td>and how to</td>
      <td>3</td>
    </tr>
    <tr>
      <th>269</th>
      <td>for image restoration</td>
      <td>3</td>
    </tr>
    <tr>
      <th>270</th>
      <td>unleashing the power</td>
      <td>3</td>
    </tr>
    <tr>
      <th>271</th>
      <td>for image generation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>272</th>
      <td>visual place recognition</td>
      <td>3</td>
    </tr>
    <tr>
      <th>273</th>
      <td>point cloud semantic</td>
      <td>3</td>
    </tr>
    <tr>
      <th>274</th>
      <td>cloud semantic segmentation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>275</th>
      <td>for human centric</td>
      <td>3</td>
    </tr>
    <tr>
      <th>276</th>
      <td>multi view 3d</td>
      <td>3</td>
    </tr>
    <tr>
      <th>277</th>
      <td>scene text recognition</td>
      <td>3</td>
    </tr>
    <tr>
      <th>278</th>
      <td>with event cameras</td>
      <td>3</td>
    </tr>
    <tr>
      <th>279</th>
      <td>propagation network for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>280</th>
      <td>multi view images</td>
      <td>3</td>
    </tr>
    <tr>
      <th>281</th>
      <td>deep neural networks</td>
      <td>3</td>
    </tr>
    <tr>
      <th>282</th>
      <td>large multimodal model</td>
      <td>3</td>
    </tr>
    <tr>
      <th>283</th>
      <td>in computational pathology</td>
      <td>3</td>
    </tr>
    <tr>
      <th>284</th>
      <td>network for 3d</td>
      <td>3</td>
    </tr>
    <tr>
      <th>285</th>
      <td>personalized federated learning</td>
      <td>3</td>
    </tr>
    <tr>
      <th>286</th>
      <td>to image personalization</td>
      <td>3</td>
    </tr>
    <tr>
      <th>287</th>
      <td>feature fusion for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>288</th>
      <td>using diffusion model</td>
      <td>3</td>
    </tr>
    <tr>
      <th>289</th>
      <td>context learning for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>290</th>
      <td>for multimodal large</td>
      <td>3</td>
    </tr>
    <tr>
      <th>291</th>
      <td>text driven 3d</td>
      <td>3</td>
    </tr>
    <tr>
      <th>292</th>
      <td>denoising diffusion models</td>
      <td>3</td>
    </tr>
    <tr>
      <th>293</th>
      <td>via diffusion models</td>
      <td>3</td>
    </tr>
    <tr>
      <th>294</th>
      <td>for non exemplar</td>
      <td>3</td>
    </tr>
    <tr>
      <th>295</th>
      <td>oriented object detection</td>
      <td>3</td>
    </tr>
    <tr>
      <th>296</th>
      <td>3d human motion</td>
      <td>3</td>
    </tr>
    <tr>
      <th>297</th>
      <td>model for visual</td>
      <td>3</td>
    </tr>
    <tr>
      <th>298</th>
      <td>for monocular depth</td>
      <td>3</td>
    </tr>
    <tr>
      <th>299</th>
      <td>large scale dataset</td>
      <td>3</td>
    </tr>
    <tr>
      <th>300</th>
      <td>for segment anything</td>
      <td>3</td>
    </tr>
    <tr>
      <th>301</th>
      <td>in federated learning</td>
      <td>3</td>
    </tr>
    <tr>
      <th>302</th>
      <td>anything model for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>303</th>
      <td>for category agnostic</td>
      <td>3</td>
    </tr>
    <tr>
      <th>304</th>
      <td>category agnostic pose</td>
      <td>3</td>
    </tr>
    <tr>
      <th>305</th>
      <td>agnostic pose estimation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>306</th>
      <td>zero shot classification</td>
      <td>3</td>
    </tr>
    <tr>
      <th>307</th>
      <td>guided diffusion for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>308</th>
      <td>via mixture of</td>
      <td>3</td>
    </tr>
    <tr>
      <th>309</th>
      <td>with diffusion model</td>
      <td>3</td>
    </tr>
    <tr>
      <th>310</th>
      <td>for text guided</td>
      <td>3</td>
    </tr>
    <tr>
      <th>311</th>
      <td>generation with text</td>
      <td>3</td>
    </tr>
    <tr>
      <th>312</th>
      <td>from monocular video</td>
      <td>3</td>
    </tr>
    <tr>
      <th>313</th>
      <td>supervised learning of</td>
      <td>3</td>
    </tr>
    <tr>
      <th>314</th>
      <td>test time training</td>
      <td>3</td>
    </tr>
    <tr>
      <th>315</th>
      <td>estimation with diffusion</td>
      <td>3</td>
    </tr>
    <tr>
      <th>316</th>
      <td>6d pose estimation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>317</th>
      <td>view 3d reconstruction</td>
      <td>3</td>
    </tr>
    <tr>
      <th>318</th>
      <td>for diffusion based</td>
      <td>3</td>
    </tr>
    <tr>
      <th>319</th>
      <td>vision transformers for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>320</th>
      <td>guided human motion</td>
      <td>3</td>
    </tr>
    <tr>
      <th>321</th>
      <td>defense against adversarial</td>
      <td>3</td>
    </tr>
    <tr>
      <th>322</th>
      <td>for low shot</td>
      <td>3</td>
    </tr>
    <tr>
      <th>323</th>
      <td>shot image classification</td>
      <td>3</td>
    </tr>
    <tr>
      <th>324</th>
      <td>object tracking with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>325</th>
      <td>for person re</td>
      <td>3</td>
    </tr>
    <tr>
      <th>326</th>
      <td>is all you</td>
      <td>3</td>
    </tr>
    <tr>
      <th>327</th>
      <td>you need for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>328</th>
      <td>blind motion deblurring</td>
      <td>3</td>
    </tr>
    <tr>
      <th>329</th>
      <td>for pre trained</td>
      <td>3</td>
    </tr>
    <tr>
      <th>330</th>
      <td>motion capture with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>331</th>
      <td>frame interpolation via</td>
      <td>3</td>
    </tr>
    <tr>
      <th>332</th>
      <td>for text based</td>
      <td>3</td>
    </tr>
    <tr>
      <th>333</th>
      <td>from point clouds</td>
      <td>3</td>
    </tr>
    <tr>
      <th>334</th>
      <td>a dataset for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>335</th>
      <td>composed image retrieval</td>
      <td>3</td>
    </tr>
    <tr>
      <th>336</th>
      <td>foundation models for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>337</th>
      <td>supervised anomaly detection</td>
      <td>3</td>
    </tr>
    <tr>
      <th>338</th>
      <td>human generation via</td>
      <td>3</td>
    </tr>
    <tr>
      <th>339</th>
      <td>in large vision</td>
      <td>3</td>
    </tr>
    <tr>
      <th>340</th>
      <td>towards high fidelity</td>
      <td>3</td>
    </tr>
    <tr>
      <th>341</th>
      <td>for test time</td>
      <td>3</td>
    </tr>
    <tr>
      <th>342</th>
      <td>for remote sensing</td>
      <td>3</td>
    </tr>
    <tr>
      <th>343</th>
      <td>semi supervised learning</td>
      <td>3</td>
    </tr>
    <tr>
      <th>344</th>
      <td>a training free</td>
      <td>3</td>
    </tr>
    <tr>
      <th>345</th>
      <td>of pre trained</td>
      <td>3</td>
    </tr>
    <tr>
      <th>346</th>
      <td>point cloud analysis</td>
      <td>3</td>
    </tr>
    <tr>
      <th>347</th>
      <td>learning for medical</td>
      <td>3</td>
    </tr>
    <tr>
      <th>348</th>
      <td>towards open vocabulary</td>
      <td>3</td>
    </tr>
    <tr>
      <th>349</th>
      <td>object detection learning</td>
      <td>3</td>
    </tr>
    <tr>
      <th>350</th>
      <td>for federated learning</td>
      <td>3</td>
    </tr>
    <tr>
      <th>351</th>
      <td>large multimodal models</td>
      <td>3</td>
    </tr>
    <tr>
      <th>352</th>
      <td>view synthesis with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>353</th>
      <td>bridging the gap</td>
      <td>3</td>
    </tr>
    <tr>
      <th>354</th>
      <td>visual language models</td>
      <td>3</td>
    </tr>
    <tr>
      <th>355</th>
      <td>optical flow estimation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>356</th>
      <td>semantic segmentation in</td>
      <td>3</td>
    </tr>
    <tr>
      <th>357</th>
      <td>scene reconstruction from</td>
      <td>3</td>
    </tr>
    <tr>
      <th>358</th>
      <td>aggregation network for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>359</th>
      <td>supervised learning for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>360</th>
      <td>multi modal large</td>
      <td>3</td>
    </tr>
    <tr>
      <th>361</th>
      <td>for real time</td>
      <td>3</td>
    </tr>
    <tr>
      <th>362</th>
      <td>for open set</td>
      <td>3</td>
    </tr>
    <tr>
      <th>363</th>
      <td>3d reconstruction with</td>
      <td>3</td>
    </tr>
    <tr>
      <th>364</th>
      <td>for vision based</td>
      <td>3</td>
    </tr>
    <tr>
      <th>365</th>
      <td>vision based 3d</td>
      <td>3</td>
    </tr>
    <tr>
      <th>366</th>
      <td>human mesh recovery</td>
      <td>3</td>
    </tr>
    <tr>
      <th>367</th>
      <td>3d occupancy prediction</td>
      <td>3</td>
    </tr>
    <tr>
      <th>368</th>
      <td>in image classification</td>
      <td>3</td>
    </tr>
    <tr>
      <th>369</th>
      <td>shot semantic segmentation</td>
      <td>3</td>
    </tr>
    <tr>
      <th>370</th>
      <td>free class incremental</td>
      <td>3</td>
    </tr>
    <tr>
      <th>371</th>
      <td>personalized text to</td>
      <td>3</td>
    </tr>
    <tr>
      <th>372</th>
      <td>human motion prediction</td>
      <td>3</td>
    </tr>
    <tr>
      <th>373</th>
      <td>for unsupervised video</td>
      <td>3</td>
    </tr>
    <tr>
      <th>374</th>
      <td>a simple yet</td>
      <td>3</td>
    </tr>
    <tr>
      <th>375</th>
      <td>using 2d diffusion</td>
      <td>3</td>
    </tr>
    <tr>
      <th>376</th>
      <td>training quantization for</td>
      <td>3</td>
    </tr>
    <tr>
      <th>377</th>
      <td>for efficient image</td>
      <td>3</td>
    </tr>
    <tr>
      <th>378</th>
      <td>via multi view</td>
      <td>3</td>
    </tr>
    <tr>
      <th>379</th>
      <td>to 3d using</td>
      <td>3</td>
    </tr>
    <tr>
      <th>380</th>
      <td>synthesis from a</td>
      <td>3</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Phrase</th>
      <th>Frequencies</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>diffusion models</td>
      <td>95</td>
    </tr>
    <tr>
      <th>1</th>
      <td>text to</td>
      <td>86</td>
    </tr>
    <tr>
      <th>2</th>
      <td>learning for</td>
      <td>68</td>
    </tr>
    <tr>
      <th>3</th>
      <td>language models</td>
      <td>67</td>
    </tr>
    <tr>
      <th>4</th>
      <td>vision language</td>
      <td>64</td>
    </tr>
    <tr>
      <th>5</th>
      <td>object detection</td>
      <td>63</td>
    </tr>
    <tr>
      <th>6</th>
      <td>to image</td>
      <td>59</td>
    </tr>
    <tr>
      <th>7</th>
      <td>semantic segmentation</td>
      <td>49</td>
    </tr>
    <tr>
      <th>8</th>
      <td>zero shot</td>
      <td>48</td>
    </tr>
    <tr>
      <th>9</th>
      <td>model for</td>
      <td>48</td>
    </tr>
    <tr>
      <th>10</th>
      <td>point cloud</td>
      <td>41</td>
    </tr>
    <tr>
      <th>11</th>
      <td>diffusion model</td>
      <td>41</td>
    </tr>
    <tr>
      <th>12</th>
      <td>models for</td>
      <td>39</td>
    </tr>
    <tr>
      <th>13</th>
      <td>multi modal</td>
      <td>39</td>
    </tr>
    <tr>
      <th>14</th>
      <td>open vocabulary</td>
      <td>37</td>
    </tr>
    <tr>
      <th>15</th>
      <td>image generation</td>
      <td>37</td>
    </tr>
    <tr>
      <th>16</th>
      <td>self supervised</td>
      <td>36</td>
    </tr>
    <tr>
      <th>17</th>
      <td>few shot</td>
      <td>36</td>
    </tr>
    <tr>
      <th>18</th>
      <td>pose estimation</td>
      <td>36</td>
    </tr>
    <tr>
      <th>19</th>
      <td>multi view</td>
      <td>35</td>
    </tr>
    <tr>
      <th>20</th>
      <td>super resolution</td>
      <td>35</td>
    </tr>
    <tr>
      <th>21</th>
      <td>for 3d</td>
      <td>35</td>
    </tr>
    <tr>
      <th>22</th>
      <td>generation with</td>
      <td>34</td>
    </tr>
    <tr>
      <th>23</th>
      <td>for text</td>
      <td>34</td>
    </tr>
    <tr>
      <th>24</th>
      <td>for image</td>
      <td>32</td>
    </tr>
    <tr>
      <th>25</th>
      <td>large language</td>
      <td>32</td>
    </tr>
    <tr>
      <th>26</th>
      <td>framework for</td>
      <td>32</td>
    </tr>
    <tr>
      <th>27</th>
      <td>3d object</td>
      <td>29</td>
    </tr>
    <tr>
      <th>28</th>
      <td>large scale</td>
      <td>29</td>
    </tr>
    <tr>
      <th>29</th>
      <td>radiance fields</td>
      <td>29</td>
    </tr>
    <tr>
      <th>30</th>
      <td>3d human</td>
      <td>28</td>
    </tr>
    <tr>
      <th>31</th>
      <td>for multi</td>
      <td>27</td>
    </tr>
    <tr>
      <th>32</th>
      <td>representation learning</td>
      <td>27</td>
    </tr>
    <tr>
      <th>33</th>
      <td>gaussian splatting</td>
      <td>27</td>
    </tr>
    <tr>
      <th>34</th>
      <td>real world</td>
      <td>26</td>
    </tr>
    <tr>
      <th>35</th>
      <td>for efficient</td>
      <td>26</td>
    </tr>
    <tr>
      <th>36</th>
      <td>test time</td>
      <td>26</td>
    </tr>
    <tr>
      <th>37</th>
      <td>network for</td>
      <td>26</td>
    </tr>
    <tr>
      <th>38</th>
      <td>from a</td>
      <td>26</td>
    </tr>
    <tr>
      <th>39</th>
      <td>transformer for</td>
      <td>25</td>
    </tr>
    <tr>
      <th>40</th>
      <td>language model</td>
      <td>25</td>
    </tr>
    <tr>
      <th>41</th>
      <td>to 3d</td>
      <td>25</td>
    </tr>
    <tr>
      <th>42</th>
      <td>weakly supervised</td>
      <td>24</td>
    </tr>
    <tr>
      <th>43</th>
      <td>for open</td>
      <td>24</td>
    </tr>
    <tr>
      <th>44</th>
      <td>learning with</td>
      <td>24</td>
    </tr>
    <tr>
      <th>45</th>
      <td>semi supervised</td>
      <td>24</td>
    </tr>
    <tr>
      <th>46</th>
      <td>3d scene</td>
      <td>23</td>
    </tr>
    <tr>
      <th>47</th>
      <td>neural radiance</td>
      <td>23</td>
    </tr>
    <tr>
      <th>48</th>
      <td>real time</td>
      <td>23</td>
    </tr>
    <tr>
      <th>49</th>
      <td>pre training</td>
      <td>22</td>
    </tr>
    <tr>
      <th>50</th>
      <td>with diffusion</td>
      <td>22</td>
    </tr>
    <tr>
      <th>51</th>
      <td>diffusion for</td>
      <td>22</td>
    </tr>
    <tr>
      <th>52</th>
      <td>autonomous driving</td>
      <td>21</td>
    </tr>
    <tr>
      <th>53</th>
      <td>image segmentation</td>
      <td>21</td>
    </tr>
    <tr>
      <th>54</th>
      <td>diffusion based</td>
      <td>21</td>
    </tr>
    <tr>
      <th>55</th>
      <td>for unsupervised</td>
      <td>21</td>
    </tr>
    <tr>
      <th>56</th>
      <td>in the</td>
      <td>21</td>
    </tr>
    <tr>
      <th>57</th>
      <td>reconstruction from</td>
      <td>21</td>
    </tr>
    <tr>
      <th>58</th>
      <td>generation via</td>
      <td>21</td>
    </tr>
    <tr>
      <th>59</th>
      <td>representation for</td>
      <td>20</td>
    </tr>
    <tr>
      <th>60</th>
      <td>high fidelity</td>
      <td>20</td>
    </tr>
    <tr>
      <th>61</th>
      <td>fine grained</td>
      <td>20</td>
    </tr>
    <tr>
      <th>62</th>
      <td>image diffusion</td>
      <td>19</td>
    </tr>
    <tr>
      <th>63</th>
      <td>contrastive learning</td>
      <td>19</td>
    </tr>
    <tr>
      <th>64</th>
      <td>end to</td>
      <td>19</td>
    </tr>
    <tr>
      <th>65</th>
      <td>to end</td>
      <td>19</td>
    </tr>
    <tr>
      <th>66</th>
      <td>single image</td>
      <td>19</td>
    </tr>
    <tr>
      <th>67</th>
      <td>audio visual</td>
      <td>18</td>
    </tr>
    <tr>
      <th>68</th>
      <td>a single</td>
      <td>18</td>
    </tr>
    <tr>
      <th>69</th>
      <td>models with</td>
      <td>18</td>
    </tr>
    <tr>
      <th>70</th>
      <td>pre trained</td>
      <td>18</td>
    </tr>
    <tr>
      <th>71</th>
      <td>for vision</td>
      <td>18</td>
    </tr>
    <tr>
      <th>72</th>
      <td>federated learning</td>
      <td>18</td>
    </tr>
    <tr>
      <th>73</th>
      <td>adaptation for</td>
      <td>18</td>
    </tr>
    <tr>
      <th>74</th>
      <td>for zero</td>
      <td>17</td>
    </tr>
    <tr>
      <th>75</th>
      <td>detection with</td>
      <td>17</td>
    </tr>
    <tr>
      <th>76</th>
      <td>a unified</td>
      <td>17</td>
    </tr>
    <tr>
      <th>77</th>
      <td>3d generation</td>
      <td>17</td>
    </tr>
    <tr>
      <th>78</th>
      <td>dataset for</td>
      <td>17</td>
    </tr>
    <tr>
      <th>79</th>
      <td>image editing</td>
      <td>17</td>
    </tr>
    <tr>
      <th>80</th>
      <td>single view</td>
      <td>17</td>
    </tr>
    <tr>
      <th>81</th>
      <td>based on</td>
      <td>17</td>
    </tr>
    <tr>
      <th>82</th>
      <td>continual learning</td>
      <td>17</td>
    </tr>
    <tr>
      <th>83</th>
      <td>learning to</td>
      <td>17</td>
    </tr>
    <tr>
      <th>84</th>
      <td>anomaly detection</td>
      <td>17</td>
    </tr>
    <tr>
      <th>85</th>
      <td>domain adaptation</td>
      <td>16</td>
    </tr>
    <tr>
      <th>86</th>
      <td>image synthesis</td>
      <td>16</td>
    </tr>
    <tr>
      <th>87</th>
      <td>supervised learning</td>
      <td>16</td>
    </tr>
    <tr>
      <th>88</th>
      <td>image super</td>
      <td>16</td>
    </tr>
    <tr>
      <th>89</th>
      <td>human pose</td>
      <td>16</td>
    </tr>
    <tr>
      <th>90</th>
      <td>in 3d</td>
      <td>16</td>
    </tr>
    <tr>
      <th>91</th>
      <td>foundation models</td>
      <td>16</td>
    </tr>
    <tr>
      <th>92</th>
      <td>3d reconstruction</td>
      <td>16</td>
    </tr>
    <tr>
      <th>93</th>
      <td>out of</td>
      <td>15</td>
    </tr>
    <tr>
      <th>94</th>
      <td>medical image</td>
      <td>15</td>
    </tr>
    <tr>
      <th>95</th>
      <td>for video</td>
      <td>15</td>
    </tr>
    <tr>
      <th>96</th>
      <td>to video</td>
      <td>15</td>
    </tr>
    <tr>
      <th>97</th>
      <td>view synthesis</td>
      <td>15</td>
    </tr>
    <tr>
      <th>98</th>
      <td>video generation</td>
      <td>15</td>
    </tr>
    <tr>
      <th>99</th>
      <td>image classification</td>
      <td>15</td>
    </tr>
    <tr>
      <th>100</th>
      <td>benchmark for</td>
      <td>14</td>
    </tr>
    <tr>
      <th>101</th>
      <td>of 3d</td>
      <td>14</td>
    </tr>
    <tr>
      <th>102</th>
      <td>for weakly</td>
      <td>14</td>
    </tr>
    <tr>
      <th>103</th>
      <td>object interaction</td>
      <td>14</td>
    </tr>
    <tr>
      <th>104</th>
      <td>open world</td>
      <td>14</td>
    </tr>
    <tr>
      <th>105</th>
      <td>image restoration</td>
      <td>14</td>
    </tr>
    <tr>
      <th>106</th>
      <td>reconstruction with</td>
      <td>14</td>
    </tr>
    <tr>
      <th>107</th>
      <td>human motion</td>
      <td>14</td>
    </tr>
    <tr>
      <th>108</th>
      <td>time adaptation</td>
      <td>14</td>
    </tr>
    <tr>
      <th>109</th>
      <td>in context</td>
      <td>14</td>
    </tr>
    <tr>
      <th>110</th>
      <td>for high</td>
      <td>14</td>
    </tr>
    <tr>
      <th>111</th>
      <td>for visual</td>
      <td>14</td>
    </tr>
    <tr>
      <th>112</th>
      <td>segment anything</td>
      <td>14</td>
    </tr>
    <tr>
      <th>113</th>
      <td>on the</td>
      <td>14</td>
    </tr>
    <tr>
      <th>114</th>
      <td>vision transformer</td>
      <td>13</td>
    </tr>
    <tr>
      <th>115</th>
      <td>estimation with</td>
      <td>13</td>
    </tr>
    <tr>
      <th>116</th>
      <td>3d gaussian</td>
      <td>13</td>
    </tr>
    <tr>
      <th>117</th>
      <td>spatio temporal</td>
      <td>13</td>
    </tr>
    <tr>
      <th>118</th>
      <td>instance segmentation</td>
      <td>13</td>
    </tr>
    <tr>
      <th>119</th>
      <td>for multimodal</td>
      <td>13</td>
    </tr>
    <tr>
      <th>120</th>
      <td>text guided</td>
      <td>13</td>
    </tr>
    <tr>
      <th>121</th>
      <td>class incremental</td>
      <td>13</td>
    </tr>
    <tr>
      <th>122</th>
      <td>knowledge distillation</td>
      <td>13</td>
    </tr>
    <tr>
      <th>123</th>
      <td>distillation for</td>
      <td>13</td>
    </tr>
    <tr>
      <th>124</th>
      <td>of diffusion</td>
      <td>13</td>
    </tr>
    <tr>
      <th>125</th>
      <td>re identification</td>
      <td>13</td>
    </tr>
    <tr>
      <th>126</th>
      <td>cross modal</td>
      <td>13</td>
    </tr>
    <tr>
      <th>127</th>
      <td>supervised semantic</td>
      <td>12</td>
    </tr>
    <tr>
      <th>128</th>
      <td>vision transformers</td>
      <td>12</td>
    </tr>
    <tr>
      <th>129</th>
      <td>of distribution</td>
      <td>12</td>
    </tr>
    <tr>
      <th>130</th>
      <td>visual grounding</td>
      <td>12</td>
    </tr>
    <tr>
      <th>131</th>
      <td>with multi</td>
      <td>12</td>
    </tr>
    <tr>
      <th>132</th>
      <td>scene understanding</td>
      <td>12</td>
    </tr>
    <tr>
      <th>133</th>
      <td>shot learning</td>
      <td>12</td>
    </tr>
    <tr>
      <th>134</th>
      <td>multi scale</td>
      <td>12</td>
    </tr>
    <tr>
      <th>135</th>
      <td>for neural</td>
      <td>12</td>
    </tr>
    <tr>
      <th>136</th>
      <td>depth estimation</td>
      <td>12</td>
    </tr>
    <tr>
      <th>137</th>
      <td>foundation model</td>
      <td>12</td>
    </tr>
    <tr>
      <th>138</th>
      <td>for diffusion</td>
      <td>12</td>
    </tr>
    <tr>
      <th>139</th>
      <td>incremental learning</td>
      <td>12</td>
    </tr>
    <tr>
      <th>140</th>
      <td>segmentation with</td>
      <td>12</td>
    </tr>
    <tr>
      <th>141</th>
      <td>3d point</td>
      <td>12</td>
    </tr>
    <tr>
      <th>142</th>
      <td>low rank</td>
      <td>12</td>
    </tr>
    <tr>
      <th>143</th>
      <td>fusion for</td>
      <td>12</td>
    </tr>
    <tr>
      <th>144</th>
      <td>in diffusion</td>
      <td>12</td>
    </tr>
    <tr>
      <th>145</th>
      <td>learning of</td>
      <td>12</td>
    </tr>
    <tr>
      <th>146</th>
      <td>person re</td>
      <td>12</td>
    </tr>
    <tr>
      <th>147</th>
      <td>domain generalization</td>
      <td>12</td>
    </tr>
    <tr>
      <th>148</th>
      <td>with large</td>
      <td>12</td>
    </tr>
    <tr>
      <th>149</th>
      <td>training for</td>
      <td>11</td>
    </tr>
    <tr>
      <th>150</th>
      <td>for robust</td>
      <td>11</td>
    </tr>
    <tr>
      <th>151</th>
      <td>generation for</td>
      <td>11</td>
    </tr>
    <tr>
      <th>152</th>
      <td>parameter efficient</td>
      <td>11</td>
    </tr>
    <tr>
      <th>153</th>
      <td>image to</td>
      <td>11</td>
    </tr>
    <tr>
      <th>154</th>
      <td>prompt learning</td>
      <td>11</td>
    </tr>
    <tr>
      <th>155</th>
      <td>for autonomous</td>
      <td>11</td>
    </tr>
    <tr>
      <th>156</th>
      <td>the wild</td>
      <td>11</td>
    </tr>
    <tr>
      <th>157</th>
      <td>neural networks</td>
      <td>11</td>
    </tr>
    <tr>
      <th>158</th>
      <td>trajectory prediction</td>
      <td>11</td>
    </tr>
    <tr>
      <th>159</th>
      <td>for human</td>
      <td>11</td>
    </tr>
    <tr>
      <th>160</th>
      <td>for few</td>
      <td>11</td>
    </tr>
    <tr>
      <th>161</th>
      <td>synthesis with</td>
      <td>11</td>
    </tr>
    <tr>
      <th>162</th>
      <td>event based</td>
      <td>11</td>
    </tr>
    <tr>
      <th>163</th>
      <td>for large</td>
      <td>11</td>
    </tr>
    <tr>
      <th>164</th>
      <td>multimodal large</td>
      <td>11</td>
    </tr>
    <tr>
      <th>165</th>
      <td>approach for</td>
      <td>11</td>
    </tr>
    <tr>
      <th>166</th>
      <td>point clouds</td>
      <td>11</td>
    </tr>
    <tr>
      <th>167</th>
      <td>3d gaussians</td>
      <td>11</td>
    </tr>
    <tr>
      <th>168</th>
      <td>high quality</td>
      <td>11</td>
    </tr>
    <tr>
      <th>169</th>
      <td>human object</td>
      <td>10</td>
    </tr>
    <tr>
      <th>170</th>
      <td>vocabulary object</td>
      <td>10</td>
    </tr>
    <tr>
      <th>171</th>
      <td>for generalizable</td>
      <td>10</td>
    </tr>
    <tr>
      <th>172</th>
      <td>in vision</td>
      <td>10</td>
    </tr>
    <tr>
      <th>173</th>
      <td>scene reconstruction</td>
      <td>10</td>
    </tr>
    <tr>
      <th>174</th>
      <td>question answering</td>
      <td>10</td>
    </tr>
    <tr>
      <th>175</th>
      <td>video diffusion</td>
      <td>10</td>
    </tr>
    <tr>
      <th>176</th>
      <td>object pose</td>
      <td>10</td>
    </tr>
    <tr>
      <th>177</th>
      <td>based image</td>
      <td>10</td>
    </tr>
    <tr>
      <th>178</th>
      <td>cross domain</td>
      <td>10</td>
    </tr>
    <tr>
      <th>179</th>
      <td>detection via</td>
      <td>10</td>
    </tr>
    <tr>
      <th>180</th>
      <td>from monocular</td>
      <td>10</td>
    </tr>
    <tr>
      <th>181</th>
      <td>quality assessment</td>
      <td>10</td>
    </tr>
    <tr>
      <th>182</th>
      <td>multi task</td>
      <td>10</td>
    </tr>
    <tr>
      <th>183</th>
      <td>generative models</td>
      <td>10</td>
    </tr>
    <tr>
      <th>184</th>
      <td>for semantic</td>
      <td>10</td>
    </tr>
    <tr>
      <th>185</th>
      <td>prompt tuning</td>
      <td>10</td>
    </tr>
    <tr>
      <th>186</th>
      <td>for event</td>
      <td>10</td>
    </tr>
    <tr>
      <th>187</th>
      <td>3d shape</td>
      <td>10</td>
    </tr>
    <tr>
      <th>188</th>
      <td>training free</td>
      <td>10</td>
    </tr>
    <tr>
      <th>189</th>
      <td>fields for</td>
      <td>10</td>
    </tr>
    <tr>
      <th>190</th>
      <td>segmentation via</td>
      <td>10</td>
    </tr>
    <tr>
      <th>191</th>
      <td>a large</td>
      <td>10</td>
    </tr>
    <tr>
      <th>192</th>
      <td>and language</td>
      <td>9</td>
    </tr>
    <tr>
      <th>193</th>
      <td>for long</td>
      <td>9</td>
    </tr>
    <tr>
      <th>194</th>
      <td>representations for</td>
      <td>9</td>
    </tr>
    <tr>
      <th>195</th>
      <td>prompting for</td>
      <td>9</td>
    </tr>
    <tr>
      <th>196</th>
      <td>guidance for</td>
      <td>9</td>
    </tr>
    <tr>
      <th>197</th>
      <td>object tracking</td>
      <td>9</td>
    </tr>
    <tr>
      <th>198</th>
      <td>for real</td>
      <td>9</td>
    </tr>
    <tr>
      <th>199</th>
      <td>scene text</td>
      <td>9</td>
    </tr>
    <tr>
      <th>200</th>
      <td>learning via</td>
      <td>9</td>
    </tr>
    <tr>
      <th>201</th>
      <td>fine tuning</td>
      <td>9</td>
    </tr>
    <tr>
      <th>202</th>
      <td>tuning for</td>
      <td>9</td>
    </tr>
    <tr>
      <th>203</th>
      <td>detection and</td>
      <td>9</td>
    </tr>
    <tr>
      <th>204</th>
      <td>novel view</td>
      <td>9</td>
    </tr>
    <tr>
      <th>205</th>
      <td>via diffusion</td>
      <td>9</td>
    </tr>
    <tr>
      <th>206</th>
      <td>transformers for</td>
      <td>9</td>
    </tr>
    <tr>
      <th>207</th>
      <td>a multi</td>
      <td>9</td>
    </tr>
    <tr>
      <th>208</th>
      <td>splatting for</td>
      <td>9</td>
    </tr>
    <tr>
      <th>209</th>
      <td>estimation via</td>
      <td>9</td>
    </tr>
    <tr>
      <th>210</th>
      <td>neural fields</td>
      <td>9</td>
    </tr>
    <tr>
      <th>211</th>
      <td>open set</td>
      <td>9</td>
    </tr>
    <tr>
      <th>212</th>
      <td>via multi</td>
      <td>9</td>
    </tr>
    <tr>
      <th>213</th>
      <td>rgb d</td>
      <td>9</td>
    </tr>
    <tr>
      <th>214</th>
      <td>a simple</td>
      <td>9</td>
    </tr>
    <tr>
      <th>215</th>
      <td>latent diffusion</td>
      <td>9</td>
    </tr>
    <tr>
      <th>216</th>
      <td>guided diffusion</td>
      <td>9</td>
    </tr>
    <tr>
      <th>217</th>
      <td>for semi</td>
      <td>9</td>
    </tr>
    <tr>
      <th>218</th>
      <td>anything model</td>
      <td>9</td>
    </tr>
    <tr>
      <th>219</th>
      <td>for face</td>
      <td>9</td>
    </tr>
    <tr>
      <th>220</th>
      <td>for object</td>
      <td>8</td>
    </tr>
    <tr>
      <th>221</th>
      <td>action recognition</td>
      <td>8</td>
    </tr>
    <tr>
      <th>222</th>
      <td>instruction tuning</td>
      <td>8</td>
    </tr>
    <tr>
      <th>223</th>
      <td>high resolution</td>
      <td>8</td>
    </tr>
    <tr>
      <th>224</th>
      <td>dataset and</td>
      <td>8</td>
    </tr>
    <tr>
      <th>225</th>
      <td>motion prediction</td>
      <td>8</td>
    </tr>
    <tr>
      <th>226</th>
      <td>structure from</td>
      <td>8</td>
    </tr>
    <tr>
      <th>227</th>
      <td>editing via</td>
      <td>8</td>
    </tr>
    <tr>
      <th>228</th>
      <td>tracking with</td>
      <td>8</td>
    </tr>
    <tr>
      <th>229</th>
      <td>for cross</td>
      <td>8</td>
    </tr>
    <tr>
      <th>230</th>
      <td>shot segmentation</td>
      <td>8</td>
    </tr>
    <tr>
      <th>231</th>
      <td>for dynamic</td>
      <td>8</td>
    </tr>
    <tr>
      <th>232</th>
      <td>video anomaly</td>
      <td>8</td>
    </tr>
    <tr>
      <th>233</th>
      <td>for enhanced</td>
      <td>8</td>
    </tr>
    <tr>
      <th>234</th>
      <td>for single</td>
      <td>8</td>
    </tr>
    <tr>
      <th>235</th>
      <td>one shot</td>
      <td>8</td>
    </tr>
    <tr>
      <th>236</th>
      <td>source free</td>
      <td>8</td>
    </tr>
    <tr>
      <th>237</th>
      <td>motion generation</td>
      <td>8</td>
    </tr>
    <tr>
      <th>238</th>
      <td>unified framework</td>
      <td>8</td>
    </tr>
    <tr>
      <th>239</th>
      <td>video understanding</td>
      <td>8</td>
    </tr>
    <tr>
      <th>240</th>
      <td>implicit neural</td>
      <td>8</td>
    </tr>
    <tr>
      <th>241</th>
      <td>generalization in</td>
      <td>8</td>
    </tr>
    <tr>
      <th>242</th>
      <td>adversarial attacks</td>
      <td>8</td>
    </tr>
    <tr>
      <th>243</th>
      <td>frame interpolation</td>
      <td>8</td>
    </tr>
    <tr>
      <th>244</th>
      <td>large vision</td>
      <td>8</td>
    </tr>
    <tr>
      <th>245</th>
      <td>scene graph</td>
      <td>7</td>
    </tr>
    <tr>
      <th>246</th>
      <td>with vision</td>
      <td>7</td>
    </tr>
    <tr>
      <th>247</th>
      <td>detection in</td>
      <td>7</td>
    </tr>
    <tr>
      <th>248</th>
      <td>in autonomous</td>
      <td>7</td>
    </tr>
    <tr>
      <th>249</th>
      <td>learning from</td>
      <td>7</td>
    </tr>
    <tr>
      <th>250</th>
      <td>cross view</td>
      <td>7</td>
    </tr>
    <tr>
      <th>251</th>
      <td>aggregation for</td>
      <td>7</td>
    </tr>
    <tr>
      <th>252</th>
      <td>modeling for</td>
      <td>7</td>
    </tr>
    <tr>
      <th>253</th>
      <td>hand object</td>
      <td>7</td>
    </tr>
    <tr>
      <th>254</th>
      <td>multi label</td>
      <td>7</td>
    </tr>
    <tr>
      <th>255</th>
      <td>for online</td>
      <td>7</td>
    </tr>
    <tr>
      <th>256</th>
      <td>flow estimation</td>
      <td>7</td>
    </tr>
    <tr>
      <th>257</th>
      <td>shot image</td>
      <td>7</td>
    </tr>
    <tr>
      <th>258</th>
      <td>from motion</td>
      <td>7</td>
    </tr>
    <tr>
      <th>259</th>
      <td>transformer with</td>
      <td>7</td>
    </tr>
    <tr>
      <th>260</th>
      <td>denoising diffusion</td>
      <td>7</td>
    </tr>
    <tr>
      <th>261</th>
      <td>image fusion</td>
      <td>7</td>
    </tr>
    <tr>
      <th>262</th>
      <td>for category</td>
      <td>7</td>
    </tr>
    <tr>
      <th>263</th>
      <td>category level</td>
      <td>7</td>
    </tr>
    <tr>
      <th>264</th>
      <td>semantic scene</td>
      <td>7</td>
    </tr>
    <tr>
      <th>265</th>
      <td>human reconstruction</td>
      <td>7</td>
    </tr>
    <tr>
      <th>266</th>
      <td>remote sensing</td>
      <td>7</td>
    </tr>
    <tr>
      <th>267</th>
      <td>for medical</td>
      <td>7</td>
    </tr>
    <tr>
      <th>268</th>
      <td>video editing</td>
      <td>7</td>
    </tr>
    <tr>
      <th>269</th>
      <td>language guided</td>
      <td>7</td>
    </tr>
    <tr>
      <th>270</th>
      <td>temporal action</td>
      <td>7</td>
    </tr>
    <tr>
      <th>271</th>
      <td>mesh reconstruction</td>
      <td>7</td>
    </tr>
    <tr>
      <th>272</th>
      <td>guided 3d</td>
      <td>7</td>
    </tr>
    <tr>
      <th>273</th>
      <td>of vision</td>
      <td>7</td>
    </tr>
    <tr>
      <th>274</th>
      <td>3d semantic</td>
      <td>7</td>
    </tr>
    <tr>
      <th>275</th>
      <td>multi person</td>
      <td>7</td>
    </tr>
    <tr>
      <th>276</th>
      <td>videos with</td>
      <td>7</td>
    </tr>
    <tr>
      <th>277</th>
      <td>with a</td>
      <td>7</td>
    </tr>
    <tr>
      <th>278</th>
      <td>scene generation</td>
      <td>7</td>
    </tr>
    <tr>
      <th>279</th>
      <td>3d scenes</td>
      <td>7</td>
    </tr>
    <tr>
      <th>280</th>
      <td>video object</td>
      <td>7</td>
    </tr>
    <tr>
      <th>281</th>
      <td>object segmentation</td>
      <td>7</td>
    </tr>
    <tr>
      <th>282</th>
      <td>in text</td>
      <td>7</td>
    </tr>
    <tr>
      <th>283</th>
      <td>3d visual</td>
      <td>7</td>
    </tr>
    <tr>
      <th>284</th>
      <td>for scene</td>
      <td>7</td>
    </tr>
    <tr>
      <th>285</th>
      <td>reconstruction of</td>
      <td>7</td>
    </tr>
    <tr>
      <th>286</th>
      <td>the power</td>
      <td>7</td>
    </tr>
    <tr>
      <th>287</th>
      <td>power of</td>
      <td>7</td>
    </tr>
    <tr>
      <th>288</th>
      <td>image models</td>
      <td>7</td>
    </tr>
    <tr>
      <th>289</th>
      <td>view 3d</td>
      <td>7</td>
    </tr>
    <tr>
      <th>290</th>
      <td>optical flow</td>
      <td>7</td>
    </tr>
    <tr>
      <th>291</th>
      <td>event cameras</td>
      <td>7</td>
    </tr>
    <tr>
      <th>292</th>
      <td>3d consistent</td>
      <td>7</td>
    </tr>
    <tr>
      <th>293</th>
      <td>multi object</td>
      <td>7</td>
    </tr>
    <tr>
      <th>294</th>
      <td>low light</td>
      <td>7</td>
    </tr>
    <tr>
      <th>295</th>
      <td>image quality</td>
      <td>7</td>
    </tr>
    <tr>
      <th>296</th>
      <td>gaussians for</td>
      <td>7</td>
    </tr>
    <tr>
      <th>297</th>
      <td>learning in</td>
      <td>7</td>
    </tr>
    <tr>
      <th>298</th>
      <td>stable diffusion</td>
      <td>7</td>
    </tr>
    <tr>
      <th>299</th>
      <td>monocular 3d</td>
      <td>7</td>
    </tr>
    <tr>
      <th>300</th>
      <td>video frame</td>
      <td>7</td>
    </tr>
    <tr>
      <th>301</th>
      <td>a new</td>
      <td>7</td>
    </tr>
    <tr>
      <th>302</th>
      <td>attention for</td>
      <td>7</td>
    </tr>
    <tr>
      <th>303</th>
      <td>based 3d</td>
      <td>7</td>
    </tr>
    <tr>
      <th>304</th>
      <td>human mesh</td>
      <td>7</td>
    </tr>
    <tr>
      <th>305</th>
      <td>graph generation</td>
      <td>6</td>
    </tr>
    <tr>
      <th>306</th>
      <td>training with</td>
      <td>6</td>
    </tr>
    <tr>
      <th>307</th>
      <td>for fine</td>
      <td>6</td>
    </tr>
    <tr>
      <th>308</th>
      <td>with visual</td>
      <td>6</td>
    </tr>
    <tr>
      <th>309</th>
      <td>vocabulary semantic</td>
      <td>6</td>
    </tr>
    <tr>
      <th>310</th>
      <td>image text</td>
      <td>6</td>
    </tr>
    <tr>
      <th>311</th>
      <td>distribution detection</td>
      <td>6</td>
    </tr>
    <tr>
      <th>312</th>
      <td>retrieval augmented</td>
      <td>6</td>
    </tr>
    <tr>
      <th>313</th>
      <td>face anti</td>
      <td>6</td>
    </tr>
    <tr>
      <th>314</th>
      <td>anti spoofing</td>
      <td>6</td>
    </tr>
    <tr>
      <th>315</th>
      <td>monocular video</td>
      <td>6</td>
    </tr>
    <tr>
      <th>316</th>
      <td>scaling up</td>
      <td>6</td>
    </tr>
    <tr>
      <th>317</th>
      <td>vision and</td>
      <td>6</td>
    </tr>
    <tr>
      <th>318</th>
      <td>with language</td>
      <td>6</td>
    </tr>
    <tr>
      <th>319</th>
      <td>prediction with</td>
      <td>6</td>
    </tr>
    <tr>
      <th>320</th>
      <td>image denoising</td>
      <td>6</td>
    </tr>
    <tr>
      <th>321</th>
      <td>using a</td>
      <td>6</td>
    </tr>
    <tr>
      <th>322</th>
      <td>image retrieval</td>
      <td>6</td>
    </tr>
    <tr>
      <th>323</th>
      <td>for blind</td>
      <td>6</td>
    </tr>
    <tr>
      <th>324</th>
      <td>photometric stereo</td>
      <td>6</td>
    </tr>
    <tr>
      <th>325</th>
      <td>and video</td>
      <td>6</td>
    </tr>
    <tr>
      <th>326</th>
      <td>in neural</td>
      <td>6</td>
    </tr>
    <tr>
      <th>327</th>
      <td>bird s</td>
      <td>6</td>
    </tr>
    <tr>
      <th>328</th>
      <td>understanding with</td>
      <td>6</td>
    </tr>
    <tr>
      <th>329</th>
      <td>human centric</td>
      <td>6</td>
    </tr>
    <tr>
      <th>330</th>
      <td>multimodal models</td>
      <td>6</td>
    </tr>
    <tr>
      <th>331</th>
      <td>in one</td>
      <td>6</td>
    </tr>
    <tr>
      <th>332</th>
      <td>knowledge from</td>
      <td>6</td>
    </tr>
    <tr>
      <th>333</th>
      <td>domain few</td>
      <td>6</td>
    </tr>
    <tr>
      <th>334</th>
      <td>driven 3d</td>
      <td>6</td>
    </tr>
    <tr>
      <th>335</th>
      <td>and 3d</td>
      <td>6</td>
    </tr>
    <tr>
      <th>336</th>
      <td>synthesis from</td>
      <td>6</td>
    </tr>
    <tr>
      <th>337</th>
      <td>geometry aware</td>
      <td>6</td>
    </tr>
    <tr>
      <th>338</th>
      <td>point based</td>
      <td>6</td>
    </tr>
    <tr>
      <th>339</th>
      <td>inverse rendering</td>
      <td>6</td>
    </tr>
    <tr>
      <th>340</th>
      <td>rethinking the</td>
      <td>6</td>
    </tr>
    <tr>
      <th>341</th>
      <td>occupancy prediction</td>
      <td>6</td>
    </tr>
    <tr>
      <th>342</th>
      <td>feature fusion</td>
      <td>6</td>
    </tr>
    <tr>
      <th>343</th>
      <td>in visual</td>
      <td>6</td>
    </tr>
    <tr>
      <th>344</th>
      <td>multi level</td>
      <td>6</td>
    </tr>
    <tr>
      <th>345</th>
      <td>of the</td>
      <td>6</td>
    </tr>
    <tr>
      <th>346</th>
      <td>and robust</td>
      <td>6</td>
    </tr>
    <tr>
      <th>347</th>
      <td>generation and</td>
      <td>6</td>
    </tr>
    <tr>
      <th>348</th>
      <td>score distillation</td>
      <td>6</td>
    </tr>
    <tr>
      <th>349</th>
      <td>with 2d</td>
      <td>6</td>
    </tr>
    <tr>
      <th>350</th>
      <td>supervised 3d</td>
      <td>6</td>
    </tr>
    <tr>
      <th>351</th>
      <td>unsupervised domain</td>
      <td>6</td>
    </tr>
    <tr>
      <th>352</th>
      <td>domain adaptive</td>
      <td>6</td>
    </tr>
    <tr>
      <th>353</th>
      <td>pruning for</td>
      <td>6</td>
    </tr>
    <tr>
      <th>354</th>
      <td>action detection</td>
      <td>6</td>
    </tr>
    <tr>
      <th>355</th>
      <td>plug and</td>
      <td>6</td>
    </tr>
    <tr>
      <th>356</th>
      <td>and play</td>
      <td>6</td>
    </tr>
    <tr>
      <th>357</th>
      <td>and accurate</td>
      <td>6</td>
    </tr>
    <tr>
      <th>358</th>
      <td>paradigm for</td>
      <td>6</td>
    </tr>
    <tr>
      <th>359</th>
      <td>to fine</td>
      <td>6</td>
    </tr>
    <tr>
      <th>360</th>
      <td>for deep</td>
      <td>6</td>
    </tr>
    <tr>
      <th>361</th>
      <td>radiance field</td>
      <td>6</td>
    </tr>
    <tr>
      <th>362</th>
      <td>how to</td>
      <td>6</td>
    </tr>
    <tr>
      <th>363</th>
      <td>for point</td>
      <td>6</td>
    </tr>
    <tr>
      <th>364</th>
      <td>self training</td>
      <td>6</td>
    </tr>
    <tr>
      <th>365</th>
      <td>sparse view</td>
      <td>6</td>
    </tr>
    <tr>
      <th>366</th>
      <td>text driven</td>
      <td>6</td>
    </tr>
    <tr>
      <th>367</th>
      <td>sampling for</td>
      <td>6</td>
    </tr>
    <tr>
      <th>368</th>
      <td>alignment and</td>
      <td>6</td>
    </tr>
    <tr>
      <th>369</th>
      <td>large multimodal</td>
      <td>6</td>
    </tr>
    <tr>
      <th>370</th>
      <td>from sparse</td>
      <td>6</td>
    </tr>
    <tr>
      <th>371</th>
      <td>prior for</td>
      <td>6</td>
    </tr>
    <tr>
      <th>372</th>
      <td>depth completion</td>
      <td>6</td>
    </tr>
    <tr>
      <th>373</th>
      <td>in real</td>
      <td>6</td>
    </tr>
    <tr>
      <th>374</th>
      <td>robustness of</td>
      <td>6</td>
    </tr>
    <tr>
      <th>375</th>
      <td>for low</td>
      <td>6</td>
    </tr>
    <tr>
      <th>376</th>
      <td>context learning</td>
      <td>6</td>
    </tr>
    <tr>
      <th>377</th>
      <td>scene completion</td>
      <td>6</td>
    </tr>
    <tr>
      <th>378</th>
      <td>for monocular</td>
      <td>6</td>
    </tr>
    <tr>
      <th>379</th>
      <td>monocular depth</td>
      <td>6</td>
    </tr>
    <tr>
      <th>380</th>
      <td>fast and</td>
      <td>6</td>
    </tr>
    <tr>
      <th>381</th>
      <td>bridging the</td>
      <td>6</td>
    </tr>
    <tr>
      <th>382</th>
      <td>of human</td>
      <td>6</td>
    </tr>
    <tr>
      <th>383</th>
      <td>for domain</td>
      <td>6</td>
    </tr>
    <tr>
      <th>384</th>
      <td>alignment for</td>
      <td>6</td>
    </tr>
    <tr>
      <th>385</th>
      <td>segmentation in</td>
      <td>6</td>
    </tr>
    <tr>
      <th>386</th>
      <td>for universal</td>
      <td>6</td>
    </tr>
    <tr>
      <th>387</th>
      <td>long term</td>
      <td>6</td>
    </tr>
    <tr>
      <th>388</th>
      <td>efficient image</td>
      <td>6</td>
    </tr>
    <tr>
      <th>389</th>
      <td>transfer learning</td>
      <td>6</td>
    </tr>
    <tr>
      <th>390</th>
      <td>reinforcement learning</td>
      <td>6</td>
    </tr>
    <tr>
      <th>391</th>
      <td>vision foundation</td>
      <td>6</td>
    </tr>
    <tr>
      <th>392</th>
      <td>quantization for</td>
      <td>6</td>
    </tr>
    <tr>
      <th>393</th>
      <td>image and</td>
      <td>5</td>
    </tr>
    <tr>
      <th>394</th>
      <td>using diffusion</td>
      <td>5</td>
    </tr>
    <tr>
      <th>395</th>
      <td>egocentric videos</td>
      <td>5</td>
    </tr>
    <tr>
      <th>396</th>
      <td>pseudo labeling</td>
      <td>5</td>
    </tr>
    <tr>
      <th>397</th>
      <td>learning by</td>
      <td>5</td>
    </tr>
    <tr>
      <th>398</th>
      <td>with sparse</td>
      <td>5</td>
    </tr>
    <tr>
      <th>399</th>
      <td>cloud registration</td>
      <td>5</td>
    </tr>
    <tr>
      <th>400</th>
      <td>masked autoencoders</td>
      <td>5</td>
    </tr>
    <tr>
      <th>401</th>
      <td>visual instruction</td>
      <td>5</td>
    </tr>
    <tr>
      <th>402</th>
      <td>stereo matching</td>
      <td>5</td>
    </tr>
    <tr>
      <th>403</th>
      <td>supervised video</td>
      <td>5</td>
    </tr>
    <tr>
      <th>404</th>
      <td>referring image</td>
      <td>5</td>
    </tr>
    <tr>
      <th>405</th>
      <td>decomposition for</td>
      <td>5</td>
    </tr>
    <tr>
      <th>406</th>
      <td>benchmark dataset</td>
      <td>5</td>
    </tr>
    <tr>
      <th>407</th>
      <td>and a</td>
      <td>5</td>
    </tr>
    <tr>
      <th>408</th>
      <td>a novel</td>
      <td>5</td>
    </tr>
    <tr>
      <th>409</th>
      <td>3d hand</td>
      <td>5</td>
    </tr>
    <tr>
      <th>410</th>
      <td>matching for</td>
      <td>5</td>
    </tr>
    <tr>
      <th>411</th>
      <td>in large</td>
      <td>5</td>
    </tr>
    <tr>
      <th>412</th>
      <td>referring expression</td>
      <td>5</td>
    </tr>
    <tr>
      <th>413</th>
      <td>and tracking</td>
      <td>5</td>
    </tr>
    <tr>
      <th>414</th>
      <td>representation with</td>
      <td>5</td>
    </tr>
    <tr>
      <th>415</th>
      <td>for motion</td>
      <td>5</td>
    </tr>
    <tr>
      <th>416</th>
      <td>and localization</td>
      <td>5</td>
    </tr>
    <tr>
      <th>417</th>
      <td>uncertainty in</td>
      <td>5</td>
    </tr>
    <tr>
      <th>418</th>
      <td>for self</td>
      <td>5</td>
    </tr>
    <tr>
      <th>419</th>
      <td>video question</td>
      <td>5</td>
    </tr>
    <tr>
      <th>420</th>
      <td>neural implicit</td>
      <td>5</td>
    </tr>
    <tr>
      <th>421</th>
      <td>and controllable</td>
      <td>5</td>
    </tr>
    <tr>
      <th>422</th>
      <td>video synthesis</td>
      <td>5</td>
    </tr>
    <tr>
      <th>423</th>
      <td>reconstruction using</td>
      <td>5</td>
    </tr>
    <tr>
      <th>424</th>
      <td>interaction detection</td>
      <td>5</td>
    </tr>
    <tr>
      <th>425</th>
      <td>networks for</td>
      <td>5</td>
    </tr>
    <tr>
      <th>426</th>
      <td>text image</td>
      <td>5</td>
    </tr>
    <tr>
      <th>427</th>
      <td>via meta</td>
      <td>5</td>
    </tr>
    <tr>
      <th>428</th>
      <td>blind image</td>
      <td>5</td>
    </tr>
    <tr>
      <th>429</th>
      <td>learning and</td>
      <td>5</td>
    </tr>
    <tr>
      <th>430</th>
      <td>from the</td>
      <td>5</td>
    </tr>
    <tr>
      <th>431</th>
      <td>for joint</td>
      <td>5</td>
    </tr>
    <tr>
      <th>432</th>
      <td>mixture of</td>
      <td>5</td>
    </tr>
    <tr>
      <th>433</th>
      <td>segmentation from</td>
      <td>5</td>
    </tr>
    <tr>
      <th>434</th>
      <td>s eye</td>
      <td>5</td>
    </tr>
    <tr>
      <th>435</th>
      <td>eye view</td>
      <td>5</td>
    </tr>
    <tr>
      <th>436</th>
      <td>vision based</td>
      <td>5</td>
    </tr>
    <tr>
      <th>437</th>
      <td>with image</td>
      <td>5</td>
    </tr>
    <tr>
      <th>438</th>
      <td>segmentation of</td>
      <td>5</td>
    </tr>
    <tr>
      <th>439</th>
      <td>unleashing the</td>
      <td>5</td>
    </tr>
    <tr>
      <th>440</th>
      <td>via hierarchical</td>
      <td>5</td>
    </tr>
    <tr>
      <th>441</th>
      <td>backdoor attacks</td>
      <td>5</td>
    </tr>
    <tr>
      <th>442</th>
      <td>all in</td>
      <td>5</td>
    </tr>
    <tr>
      <th>443</th>
      <td>pose and</td>
      <td>5</td>
    </tr>
    <tr>
      <th>444</th>
      <td>and shape</td>
      <td>5</td>
    </tr>
    <tr>
      <th>445</th>
      <td>image registration</td>
      <td>5</td>
    </tr>
    <tr>
      <th>446</th>
      <td>editing with</td>
      <td>5</td>
    </tr>
    <tr>
      <th>447</th>
      <td>3d facial</td>
      <td>5</td>
    </tr>
    <tr>
      <th>448</th>
      <td>video language</td>
      <td>5</td>
    </tr>
    <tr>
      <th>449</th>
      <td>a dataset</td>
      <td>5</td>
    </tr>
    <tr>
      <th>450</th>
      <td>and benchmark</td>
      <td>5</td>
    </tr>
    <tr>
      <th>451</th>
      <td>control for</td>
      <td>5</td>
    </tr>
    <tr>
      <th>452</th>
      <td>synthetic data</td>
      <td>5</td>
    </tr>
    <tr>
      <th>453</th>
      <td>fusion in</td>
      <td>5</td>
    </tr>
    <tr>
      <th>454</th>
      <td>segmentation by</td>
      <td>5</td>
    </tr>
    <tr>
      <th>455</th>
      <td>fields from</td>
      <td>5</td>
    </tr>
    <tr>
      <th>456</th>
      <td>fields with</td>
      <td>5</td>
    </tr>
    <tr>
      <th>457</th>
      <td>whole slide</td>
      <td>5</td>
    </tr>
    <tr>
      <th>458</th>
      <td>video quality</td>
      <td>5</td>
    </tr>
    <tr>
      <th>459</th>
      <td>indoor scenes</td>
      <td>5</td>
    </tr>
    <tr>
      <th>460</th>
      <td>for camera</td>
      <td>5</td>
    </tr>
    <tr>
      <th>461</th>
      <td>in federated</td>
      <td>5</td>
    </tr>
    <tr>
      <th>462</th>
      <td>generation using</td>
      <td>5</td>
    </tr>
    <tr>
      <th>463</th>
      <td>model with</td>
      <td>5</td>
    </tr>
    <tr>
      <th>464</th>
      <td>2d 3d</td>
      <td>5</td>
    </tr>
    <tr>
      <th>465</th>
      <td>and efficient</td>
      <td>5</td>
    </tr>
    <tr>
      <th>466</th>
      <td>encoding for</td>
      <td>5</td>
    </tr>
    <tr>
      <th>467</th>
      <td>vision models</td>
      <td>5</td>
    </tr>
    <tr>
      <th>468</th>
      <td>models via</td>
      <td>5</td>
    </tr>
    <tr>
      <th>469</th>
      <td>coarse to</td>
      <td>5</td>
    </tr>
    <tr>
      <th>470</th>
      <td>with foundation</td>
      <td>5</td>
    </tr>
    <tr>
      <th>471</th>
      <td>object centric</td>
      <td>5</td>
    </tr>
    <tr>
      <th>472</th>
      <td>accurate and</td>
      <td>5</td>
    </tr>
    <tr>
      <th>473</th>
      <td>memory efficient</td>
      <td>5</td>
    </tr>
    <tr>
      <th>474</th>
      <td>for sparse</td>
      <td>5</td>
    </tr>
    <tr>
      <th>475</th>
      <td>unsupervised video</td>
      <td>5</td>
    </tr>
    <tr>
      <th>476</th>
      <td>free domain</td>
      <td>5</td>
    </tr>
    <tr>
      <th>477</th>
      <td>from text</td>
      <td>5</td>
    </tr>
    <tr>
      <th>478</th>
      <td>an empirical</td>
      <td>5</td>
    </tr>
    <tr>
      <th>479</th>
      <td>empirical study</td>
      <td>5</td>
    </tr>
    <tr>
      <th>480</th>
      <td>estimation for</td>
      <td>5</td>
    </tr>
    <tr>
      <th>481</th>
      <td>3d pose</td>
      <td>5</td>
    </tr>
    <tr>
      <th>482</th>
      <td>models by</td>
      <td>5</td>
    </tr>
    <tr>
      <th>483</th>
      <td>action segmentation</td>
      <td>5</td>
    </tr>
    <tr>
      <th>484</th>
      <td>adversarial robustness</td>
      <td>5</td>
    </tr>
    <tr>
      <th>485</th>
      <td>adversarial training</td>
      <td>5</td>
    </tr>
    <tr>
      <th>486</th>
      <td>clip for</td>
      <td>5</td>
    </tr>
    <tr>
      <th>487</th>
      <td>defense against</td>
      <td>5</td>
    </tr>
    <tr>
      <th>488</th>
      <td>of text</td>
      <td>5</td>
    </tr>
    <tr>
      <th>489</th>
      <td>co speech</td>
      <td>5</td>
    </tr>
    <tr>
      <th>490</th>
      <td>the robustness</td>
      <td>5</td>
    </tr>
    <tr>
      <th>491</th>
      <td>rendering of</td>
      <td>5</td>
    </tr>
    <tr>
      <th>492</th>
      <td>with dynamic</td>
      <td>5</td>
    </tr>
    <tr>
      <th>493</th>
      <td>human feedback</td>
      <td>5</td>
    </tr>
    <tr>
      <th>494</th>
      <td>efficient and</td>
      <td>5</td>
    </tr>
    <tr>
      <th>495</th>
      <td>aware multi</td>
      <td>5</td>
    </tr>
    <tr>
      <th>496</th>
      <td>motion capture</td>
      <td>5</td>
    </tr>
    <tr>
      <th>497</th>
      <td>matching with</td>
      <td>5</td>
    </tr>
    <tr>
      <th>498</th>
      <td>deepfake detection</td>
      <td>5</td>
    </tr>
    <tr>
      <th>499</th>
      <td>visual perception</td>
      <td>5</td>
    </tr>
    <tr>
      <th>500</th>
      <td>virtual try</td>
      <td>5</td>
    </tr>
    <tr>
      <th>501</th>
      <td>try on</td>
      <td>5</td>
    </tr>
    <tr>
      <th>502</th>
      <td>data from</td>
      <td>5</td>
    </tr>
    <tr>
      <th>503</th>
      <td>with text</td>
      <td>5</td>
    </tr>
    <tr>
      <th>504</th>
      <td>is worth</td>
      <td>5</td>
    </tr>
    <tr>
      <th>505</th>
      <td>6d object</td>
      <td>5</td>
    </tr>
    <tr>
      <th>506</th>
      <td>neural representation</td>
      <td>5</td>
    </tr>
    <tr>
      <th>507</th>
      <td>for referring</td>
      <td>5</td>
    </tr>
    <tr>
      <th>508</th>
      <td>diffusion priors</td>
      <td>5</td>
    </tr>
    <tr>
      <th>509</th>
      <td>of large</td>
      <td>5</td>
    </tr>
    <tr>
      <th>510</th>
      <td>all you</td>
      <td>5</td>
    </tr>
    <tr>
      <th>511</th>
      <td>you need</td>
      <td>5</td>
    </tr>
    <tr>
      <th>512</th>
      <td>human generation</td>
      <td>5</td>
    </tr>
    <tr>
      <th>513</th>
      <td>vocabulary 3d</td>
      <td>5</td>
    </tr>
    <tr>
      <th>514</th>
      <td>image analysis</td>
      <td>5</td>
    </tr>
    <tr>
      <th>515</th>
      <td>regularization for</td>
      <td>5</td>
    </tr>
    <tr>
      <th>516</th>
      <td>visual language</td>
      <td>5</td>
    </tr>
    <tr>
      <th>517</th>
      <td>language driven</td>
      <td>5</td>
    </tr>
    <tr>
      <th>518</th>
      <td>2d diffusion</td>
      <td>5</td>
    </tr>
    <tr>
      <th>519</th>
      <td>post training</td>
      <td>5</td>
    </tr>
    <tr>
      <th>520</th>
      <td>scene graphs</td>
      <td>4</td>
    </tr>
    <tr>
      <th>521</th>
      <td>x ray</td>
      <td>4</td>
    </tr>
    <tr>
      <th>522</th>
      <td>data efficient</td>
      <td>4</td>
    </tr>
    <tr>
      <th>523</th>
      <td>for audio</td>
      <td>4</td>
    </tr>
    <tr>
      <th>524</th>
      <td>copyright protection</td>
      <td>4</td>
    </tr>
    <tr>
      <th>525</th>
      <td>adaptation with</td>
      <td>4</td>
    </tr>
    <tr>
      <th>526</th>
      <td>prediction for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>527</th>
      <td>and cross</td>
      <td>4</td>
    </tr>
    <tr>
      <th>528</th>
      <td>for stereo</td>
      <td>4</td>
    </tr>
    <tr>
      <th>529</th>
      <td>field for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>530</th>
      <td>for active</td>
      <td>4</td>
    </tr>
    <tr>
      <th>531</th>
      <td>based visual</td>
      <td>4</td>
    </tr>
    <tr>
      <th>532</th>
      <td>visual segmentation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>533</th>
      <td>conditional diffusion</td>
      <td>4</td>
    </tr>
    <tr>
      <th>534</th>
      <td>and adaptive</td>
      <td>4</td>
    </tr>
    <tr>
      <th>535</th>
      <td>of thought</td>
      <td>4</td>
    </tr>
    <tr>
      <th>536</th>
      <td>panoptic segmentation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>537</th>
      <td>trained models</td>
      <td>4</td>
    </tr>
    <tr>
      <th>538</th>
      <td>diffusion framework</td>
      <td>4</td>
    </tr>
    <tr>
      <th>539</th>
      <td>image manipulation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>540</th>
      <td>ground truth</td>
      <td>4</td>
    </tr>
    <tr>
      <th>541</th>
      <td>visual representation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>542</th>
      <td>language pretraining</td>
      <td>4</td>
    </tr>
    <tr>
      <th>543</th>
      <td>multi modality</td>
      <td>4</td>
    </tr>
    <tr>
      <th>544</th>
      <td>and its</td>
      <td>4</td>
    </tr>
    <tr>
      <th>545</th>
      <td>a real</td>
      <td>4</td>
    </tr>
    <tr>
      <th>546</th>
      <td>natural language</td>
      <td>4</td>
    </tr>
    <tr>
      <th>547</th>
      <td>neural field</td>
      <td>4</td>
    </tr>
    <tr>
      <th>548</th>
      <td>hyperspectral image</td>
      <td>4</td>
    </tr>
    <tr>
      <th>549</th>
      <td>inversion for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>550</th>
      <td>distillation via</td>
      <td>4</td>
    </tr>
    <tr>
      <th>551</th>
      <td>for out</td>
      <td>4</td>
    </tr>
    <tr>
      <th>552</th>
      <td>video captioning</td>
      <td>4</td>
    </tr>
    <tr>
      <th>553</th>
      <td>synthesis for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>554</th>
      <td>detection for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>555</th>
      <td>approach to</td>
      <td>4</td>
    </tr>
    <tr>
      <th>556</th>
      <td>subject driven</td>
      <td>4</td>
    </tr>
    <tr>
      <th>557</th>
      <td>meta learning</td>
      <td>4</td>
    </tr>
    <tr>
      <th>558</th>
      <td>reconstruction via</td>
      <td>4</td>
    </tr>
    <tr>
      <th>559</th>
      <td>retrieval and</td>
      <td>4</td>
    </tr>
    <tr>
      <th>560</th>
      <td>with semantic</td>
      <td>4</td>
    </tr>
    <tr>
      <th>561</th>
      <td>instruction guided</td>
      <td>4</td>
    </tr>
    <tr>
      <th>562</th>
      <td>avatar from</td>
      <td>4</td>
    </tr>
    <tr>
      <th>563</th>
      <td>towards universal</td>
      <td>4</td>
    </tr>
    <tr>
      <th>564</th>
      <td>based semantic</td>
      <td>4</td>
    </tr>
    <tr>
      <th>565</th>
      <td>long range</td>
      <td>4</td>
    </tr>
    <tr>
      <th>566</th>
      <td>tracking in</td>
      <td>4</td>
    </tr>
    <tr>
      <th>567</th>
      <td>from 2d</td>
      <td>4</td>
    </tr>
    <tr>
      <th>568</th>
      <td>fidelity text</td>
      <td>4</td>
    </tr>
    <tr>
      <th>569</th>
      <td>one image</td>
      <td>4</td>
    </tr>
    <tr>
      <th>570</th>
      <td>for continual</td>
      <td>4</td>
    </tr>
    <tr>
      <th>571</th>
      <td>at scale</td>
      <td>4</td>
    </tr>
    <tr>
      <th>572</th>
      <td>models are</td>
      <td>4</td>
    </tr>
    <tr>
      <th>573</th>
      <td>exploring the</td>
      <td>4</td>
    </tr>
    <tr>
      <th>574</th>
      <td>for end</td>
      <td>4</td>
    </tr>
    <tr>
      <th>575</th>
      <td>data for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>576</th>
      <td>with cross</td>
      <td>4</td>
    </tr>
    <tr>
      <th>577</th>
      <td>domain generalized</td>
      <td>4</td>
    </tr>
    <tr>
      <th>578</th>
      <td>detection a</td>
      <td>4</td>
    </tr>
    <tr>
      <th>579</th>
      <td>face recognition</td>
      <td>4</td>
    </tr>
    <tr>
      <th>580</th>
      <td>to real</td>
      <td>4</td>
    </tr>
    <tr>
      <th>581</th>
      <td>for interactive</td>
      <td>4</td>
    </tr>
    <tr>
      <th>582</th>
      <td>chain of</td>
      <td>4</td>
    </tr>
    <tr>
      <th>583</th>
      <td>and how</td>
      <td>4</td>
    </tr>
    <tr>
      <th>584</th>
      <td>a comprehensive</td>
      <td>4</td>
    </tr>
    <tr>
      <th>585</th>
      <td>online continual</td>
      <td>4</td>
    </tr>
    <tr>
      <th>586</th>
      <td>transfer with</td>
      <td>4</td>
    </tr>
    <tr>
      <th>587</th>
      <td>free 3d</td>
      <td>4</td>
    </tr>
    <tr>
      <th>588</th>
      <td>of neural</td>
      <td>4</td>
    </tr>
    <tr>
      <th>589</th>
      <td>diffusion features</td>
      <td>4</td>
    </tr>
    <tr>
      <th>590</th>
      <td>towards high</td>
      <td>4</td>
    </tr>
    <tr>
      <th>591</th>
      <td>slide image</td>
      <td>4</td>
    </tr>
    <tr>
      <th>592</th>
      <td>object reconstruction</td>
      <td>4</td>
    </tr>
    <tr>
      <th>593</th>
      <td>a benchmark</td>
      <td>4</td>
    </tr>
    <tr>
      <th>594</th>
      <td>neural representations</td>
      <td>4</td>
    </tr>
    <tr>
      <th>595</th>
      <td>don t</td>
      <td>4</td>
    </tr>
    <tr>
      <th>596</th>
      <td>robust 3d</td>
      <td>4</td>
    </tr>
    <tr>
      <th>597</th>
      <td>clothed human</td>
      <td>4</td>
    </tr>
    <tr>
      <th>598</th>
      <td>a generalist</td>
      <td>4</td>
    </tr>
    <tr>
      <th>599</th>
      <td>distance fields</td>
      <td>4</td>
    </tr>
    <tr>
      <th>600</th>
      <td>3d instance</td>
      <td>4</td>
    </tr>
    <tr>
      <th>601</th>
      <td>for indoor</td>
      <td>4</td>
    </tr>
    <tr>
      <th>602</th>
      <td>with neural</td>
      <td>4</td>
    </tr>
    <tr>
      <th>603</th>
      <td>efficient multi</td>
      <td>4</td>
    </tr>
    <tr>
      <th>604</th>
      <td>generation from</td>
      <td>4</td>
    </tr>
    <tr>
      <th>605</th>
      <td>instance aware</td>
      <td>4</td>
    </tr>
    <tr>
      <th>606</th>
      <td>based object</td>
      <td>4</td>
    </tr>
    <tr>
      <th>607</th>
      <td>object detectors</td>
      <td>4</td>
    </tr>
    <tr>
      <th>608</th>
      <td>text based</td>
      <td>4</td>
    </tr>
    <tr>
      <th>609</th>
      <td>surface reconstruction</td>
      <td>4</td>
    </tr>
    <tr>
      <th>610</th>
      <td>a versatile</td>
      <td>4</td>
    </tr>
    <tr>
      <th>611</th>
      <td>what you</td>
      <td>4</td>
    </tr>
    <tr>
      <th>612</th>
      <td>you want</td>
      <td>4</td>
    </tr>
    <tr>
      <th>613</th>
      <td>dynamic and</td>
      <td>4</td>
    </tr>
    <tr>
      <th>614</th>
      <td>models a</td>
      <td>4</td>
    </tr>
    <tr>
      <th>615</th>
      <td>event camera</td>
      <td>4</td>
    </tr>
    <tr>
      <th>616</th>
      <td>language pre</td>
      <td>4</td>
    </tr>
    <tr>
      <th>617</th>
      <td>understanding and</td>
      <td>4</td>
    </tr>
    <tr>
      <th>618</th>
      <td>neural network</td>
      <td>4</td>
    </tr>
    <tr>
      <th>619</th>
      <td>medical images</td>
      <td>4</td>
    </tr>
    <tr>
      <th>620</th>
      <td>and mitigating</td>
      <td>4</td>
    </tr>
    <tr>
      <th>621</th>
      <td>architecture search</td>
      <td>4</td>
    </tr>
    <tr>
      <th>622</th>
      <td>with dual</td>
      <td>4</td>
    </tr>
    <tr>
      <th>623</th>
      <td>collaborative learning</td>
      <td>4</td>
    </tr>
    <tr>
      <th>624</th>
      <td>head avatar</td>
      <td>4</td>
    </tr>
    <tr>
      <th>625</th>
      <td>segment any</td>
      <td>4</td>
    </tr>
    <tr>
      <th>626</th>
      <td>and deformation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>627</th>
      <td>image reconstruction</td>
      <td>4</td>
    </tr>
    <tr>
      <th>628</th>
      <td>towards a</td>
      <td>4</td>
    </tr>
    <tr>
      <th>629</th>
      <td>place recognition</td>
      <td>4</td>
    </tr>
    <tr>
      <th>630</th>
      <td>dynamic 3d</td>
      <td>4</td>
    </tr>
    <tr>
      <th>631</th>
      <td>data free</td>
      <td>4</td>
    </tr>
    <tr>
      <th>632</th>
      <td>with efficient</td>
      <td>4</td>
    </tr>
    <tr>
      <th>633</th>
      <td>modal models</td>
      <td>4</td>
    </tr>
    <tr>
      <th>634</th>
      <td>study of</td>
      <td>4</td>
    </tr>
    <tr>
      <th>635</th>
      <td>for egocentric</td>
      <td>4</td>
    </tr>
    <tr>
      <th>636</th>
      <td>view images</td>
      <td>4</td>
    </tr>
    <tr>
      <th>637</th>
      <td>deep neural</td>
      <td>4</td>
    </tr>
    <tr>
      <th>638</th>
      <td>transformer based</td>
      <td>4</td>
    </tr>
    <tr>
      <th>639</th>
      <td>non rigid</td>
      <td>4</td>
    </tr>
    <tr>
      <th>640</th>
      <td>speech gesture</td>
      <td>4</td>
    </tr>
    <tr>
      <th>641</th>
      <td>language navigation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>642</th>
      <td>semantic correspondence</td>
      <td>4</td>
    </tr>
    <tr>
      <th>643</th>
      <td>method for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>644</th>
      <td>efficient video</td>
      <td>4</td>
    </tr>
    <tr>
      <th>645</th>
      <td>shot video</td>
      <td>4</td>
    </tr>
    <tr>
      <th>646</th>
      <td>transfer for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>647</th>
      <td>estimation from</td>
      <td>4</td>
    </tr>
    <tr>
      <th>648</th>
      <td>and geometric</td>
      <td>4</td>
    </tr>
    <tr>
      <th>649</th>
      <td>attacks on</td>
      <td>4</td>
    </tr>
    <tr>
      <th>650</th>
      <td>3d vision</td>
      <td>4</td>
    </tr>
    <tr>
      <th>651</th>
      <td>optimization for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>652</th>
      <td>world 3d</td>
      <td>4</td>
    </tr>
    <tr>
      <th>653</th>
      <td>in image</td>
      <td>4</td>
    </tr>
    <tr>
      <th>654</th>
      <td>for non</td>
      <td>4</td>
    </tr>
    <tr>
      <th>655</th>
      <td>category agnostic</td>
      <td>4</td>
    </tr>
    <tr>
      <th>656</th>
      <td>self distillation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>657</th>
      <td>model based</td>
      <td>4</td>
    </tr>
    <tr>
      <th>658</th>
      <td>in multi</td>
      <td>4</td>
    </tr>
    <tr>
      <th>659</th>
      <td>shape matching</td>
      <td>4</td>
    </tr>
    <tr>
      <th>660</th>
      <td>using neural</td>
      <td>4</td>
    </tr>
    <tr>
      <th>661</th>
      <td>learning multi</td>
      <td>4</td>
    </tr>
    <tr>
      <th>662</th>
      <td>2d and</td>
      <td>4</td>
    </tr>
    <tr>
      <th>663</th>
      <td>segmentation through</td>
      <td>4</td>
    </tr>
    <tr>
      <th>664</th>
      <td>light enhancement</td>
      <td>4</td>
    </tr>
    <tr>
      <th>665</th>
      <td>shot classification</td>
      <td>4</td>
    </tr>
    <tr>
      <th>666</th>
      <td>classification with</td>
      <td>4</td>
    </tr>
    <tr>
      <th>667</th>
      <td>for generative</td>
      <td>4</td>
    </tr>
    <tr>
      <th>668</th>
      <td>motion deblurring</td>
      <td>4</td>
    </tr>
    <tr>
      <th>669</th>
      <td>visual speech</td>
      <td>4</td>
    </tr>
    <tr>
      <th>670</th>
      <td>6d pose</td>
      <td>4</td>
    </tr>
    <tr>
      <th>671</th>
      <td>dataset with</td>
      <td>4</td>
    </tr>
    <tr>
      <th>672</th>
      <td>objects in</td>
      <td>4</td>
    </tr>
    <tr>
      <th>673</th>
      <td>models and</td>
      <td>4</td>
    </tr>
    <tr>
      <th>674</th>
      <td>features for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>675</th>
      <td>bias in</td>
      <td>4</td>
    </tr>
    <tr>
      <th>676</th>
      <td>avatars with</td>
      <td>4</td>
    </tr>
    <tr>
      <th>677</th>
      <td>monocular videos</td>
      <td>4</td>
    </tr>
    <tr>
      <th>678</th>
      <td>generative 3d</td>
      <td>4</td>
    </tr>
    <tr>
      <th>679</th>
      <td>for pre</td>
      <td>4</td>
    </tr>
    <tr>
      <th>680</th>
      <td>trained vision</td>
      <td>4</td>
    </tr>
    <tr>
      <th>681</th>
      <td>based human</td>
      <td>4</td>
    </tr>
    <tr>
      <th>682</th>
      <td>from point</td>
      <td>4</td>
    </tr>
    <tr>
      <th>683</th>
      <td>by learning</td>
      <td>4</td>
    </tr>
    <tr>
      <th>684</th>
      <td>correspondence learning</td>
      <td>4</td>
    </tr>
    <tr>
      <th>685</th>
      <td>human avatar</td>
      <td>4</td>
    </tr>
    <tr>
      <th>686</th>
      <td>for action</td>
      <td>4</td>
    </tr>
    <tr>
      <th>687</th>
      <td>visual prompt</td>
      <td>4</td>
    </tr>
    <tr>
      <th>688</th>
      <td>convolution for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>689</th>
      <td>scenes using</td>
      <td>4</td>
    </tr>
    <tr>
      <th>690</th>
      <td>video super</td>
      <td>4</td>
    </tr>
    <tr>
      <th>691</th>
      <td>embeddings for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>692</th>
      <td>optimal transport</td>
      <td>4</td>
    </tr>
    <tr>
      <th>693</th>
      <td>maps for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>694</th>
      <td>for federated</td>
      <td>4</td>
    </tr>
    <tr>
      <th>695</th>
      <td>shot semantic</td>
      <td>4</td>
    </tr>
    <tr>
      <th>696</th>
      <td>relative pose</td>
      <td>4</td>
    </tr>
    <tr>
      <th>697</th>
      <td>to 4d</td>
      <td>4</td>
    </tr>
    <tr>
      <th>698</th>
      <td>supervised monocular</td>
      <td>4</td>
    </tr>
    <tr>
      <th>699</th>
      <td>and multi</td>
      <td>4</td>
    </tr>
    <tr>
      <th>700</th>
      <td>augmentation for</td>
      <td>4</td>
    </tr>
    <tr>
      <th>701</th>
      <td>shape reconstruction</td>
      <td>4</td>
    </tr>
    <tr>
      <th>702</th>
      <td>recognition in</td>
      <td>4</td>
    </tr>
    <tr>
      <th>703</th>
      <td>generalized category</td>
      <td>4</td>
    </tr>
    <tr>
      <th>704</th>
      <td>category discovery</td>
      <td>4</td>
    </tr>
    <tr>
      <th>705</th>
      <td>3d motion</td>
      <td>4</td>
    </tr>
    <tr>
      <th>706</th>
      <td>neural surface</td>
      <td>4</td>
    </tr>
    <tr>
      <th>707</th>
      <td>motion estimation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>708</th>
      <td>efficient 3d</td>
      <td>4</td>
    </tr>
    <tr>
      <th>709</th>
      <td>reconstruction and</td>
      <td>4</td>
    </tr>
    <tr>
      <th>710</th>
      <td>using 2d</td>
      <td>4</td>
    </tr>
    <tr>
      <th>711</th>
      <td>training quantization</td>
      <td>4</td>
    </tr>
    <tr>
      <th>712</th>
      <td>with masked</td>
      <td>4</td>
    </tr>
  </tbody>
</table>